\documentclass{extarticle}
\sloppy
\input{packages.tex}
\input{math_commands.tex}

\title{\vspace{-2em}Chapter 6: Inner Product Spaces}
\author{\emph{Linear Algebra Done Right (4th Edition)}, by Sheldon Axler}
\date{Last updated: \today}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 6A %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{6A: Inner Products and Norms}
\addcontentsline{toc}{section}{6A: Inner Products and Norms}

\begin{definition}[dot product]
    For \(x, y \in \R^n\), the \textbf{dot product} of \(x\) and \(y\), denoted by
    \(x \cdot y\), is defined by
    \[x \cdot y = x_1 y_1 + \cdots + x_n y_n,\]
    where \(x = (x_1, \ldots, x_n)\) and \(y = (y_1, \ldots, y_n)\).
\end{definition}

\begin{definition}[inner product]
    An \textbf{inner product} on \(V\) is a function that takes each ordered pair \((u, v)\) of elements of
    \(V\) to a number \(\langle u,v \rangle \in \F\) and has the following properties:
    \begin{enumerate}[label=(\alph*)]
        \item \textbf{positivity}: \(\langle v,v \rangle \geq 0\) for all  \(v \in V\).
        \item \textbf{definiteness}: \(\langle v,v \rangle = 0\) if and only if \(v = 0\).
        \item \textbf{additivity in first slot}: \(\langle u+v,w \rangle = \langle u, w\rangle
        + \langle v,w \rangle\) for all \(u, v, w \in V\).
        \item \textbf{homogeneity in first slot}: \(\langle \lambda u,v \rangle
        = \lambda \langle u,v \rangle\) for all \(\lambda \in \F\) and all \(u, v \in V\).
        \item \textbf{conjugate symmetry}: \(\langle u,v \rangle = \overline{\langle v,u\rangle}\)
        for all \(u, v \in V\).
    \end{enumerate}
\end{definition}

\begin{definition}[inner product space]
    An \textbf{inner product space} is a vector space \(V\) along with an inner product on \(V\).
\end{definition}

\begin{corollary}[basic properties of an inner product]
    \begin{enumerate}[label=(\alph*)]
        \item For each fixed \(v \in V\), the function that takes \(u \in V\) to
        \(\langle u,v \rangle\) is a linear map from \(V\) to \(\F\).
        \item \(\langle 0,v \rangle = 0\) for every \(v \in V\).
        \item \(\langle v,0 \rangle = 0\) for every \(v \in V\).
        \item \(\langle u,v + w \rangle = \langle u,v \rangle + \langle u,w \rangle\)
        for all \(u, v, w \in V\).
        \item \(\langle u,\lambda v \rangle = \overline{\lambda} \langle u,v \rangle\)
        for all \(\lambda \in \F\) and \(u, v \in V\).
    \end{enumerate}
\end{corollary}

\begin{definition}[norm, \(\norm{v}\)]
    For \(v \in V\), the \textbf{norm} of \(v\), denoted by \(\norm{v}\), is defined by
    \[\norm{v} = \sqrt{\langle v,v \rangle}\]
\end{definition}

\begin{corollary}[basic properties of the norm]
    Suppose \(v \in V\).
    \begin{enumerate}[label=(\alph*)]
        \item \(\norm{v} = 0\) if and only if \(v = 0\).
        \item \(\norm{\lambda v} = |\lambda| \norm{v}\) for all \(\lambda \in \F\).
    \end{enumerate}
\end{corollary}

\begin{remark}
    Working with norms squared is usually easier than working directly with norms.
\end{remark}

\begin{definition}[orthogonal]
    Two vectors \(u, v \in V\) are called \textbf{orthogonal} if \(\langle u,v \rangle = 0\).
\end{definition}

\begin{corollary}[orthogonality and 0]
    \begin{enumerate}[label=(\alph*)]
        \item 0 is orthogonal to every vector in \(V\).
        \item 0 is the only vector in \(V\) that is orthogonal to itself.
    \end{enumerate}
\end{corollary}

\begin{thm}[Pythagorean Theorem]
    Suppose \(u, v \in V\). If \(u\) and \(v\) are orthogonal, then
    \[\norm{u + v}^2 = \norm{u}^2 + \norm{v}^2\]
\end{thm}

\begin{lemma}[orthogonal decomposition]
    Suppose \( u, v \in V\), with \(v \neq 0\). Set \(c = \frac{\langle u,v \rangle}{\norm{v}^2}\) and
    \(w = u - \frac{\langle u, v, \rangle}{\norm{v}^2}v\). Then
    \[u = cv+w \ \text{ and } \ \langle w,v \rangle = 0.\]
\end{lemma}

\begin{thm}[Cauchy-Schwarz inequality]
    Suppose \(u, v \in V\). Then
    \[|\langle u,v \rangle| \leq \norm{ u } \norm{ v }\]
    This inequality is an equality if and only if one of \(u, v\) is a scalar multiple of the other.
\end{thm}

\begin{thm}[triangle inequality]
    Suppose \(u, v \in V\). Then
    \[\norm{ u+v }\leq \norm{ u } + \norm{ v }.\]
    This inequality is an equality if and only if one of \(u, v\) is a nonnegative real multiple of the other.
\end{thm}

\begin{thm}[parallelogram equality]
    Suppose \(u, v \in V\). Then
    \[\norm{ u +v }^2 + \norm{ u-v }^2 = 2(\norm{u}^2 + \norm{v}^2).\]
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 6A PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{6A Problem Sets}

\begin{problem}{1}
    Prove or give a counter example: If \(v_1, \ldots, v_m \in V\), then
    \[\sum_{j=1}^{m}\sum_{k=1}^{m} \langle v_j,v_k \rangle \geq 0.\]
\end{problem}

\begin{proof}
By linearity of inner products,
\[\sum_{j=1}^{m}\sum_{k=1}^{m} \langle v_j,v_k \rangle
= \left\langle \sum_{j=1}^{m} v_j, \sum_{k=1}^{m} v_k \right\rangle \geq 0\]

since the two terms equal other and the conclusion follows by positivity of inner products.
\end{proof}

\begin{problem}{2}
    Suppose \(S \in \Lc(V)\). Define \(\langle \cdot, \cdot \rangle_1\) by
    \[\langle u,v \rangle_1 = \langle Su,Sv \rangle\]
    for all \(u, v \in V\). Show that \(\langle \cdot,\cdot \rangle_1\) is an inner product on
    \(V\) if and only if \(S\) is injective.
\end{problem}

\begin{proof}
    \(\langle \cdot,\cdot \rangle_1\) is inner product \(\Longleftrightarrow\)
    \(\langle v,v \rangle_1 = \langle Sv,Sv \rangle = 0\) if and only if \(v = 0\) \(\Longleftrightarrow\)
    \(S\) is injective. (Other properties are omitted for checking)
\end{proof}

\begin{problem}{3}
    \begin{enumerate}[label=(\alph*)]
        \item Show that the function taking an ordered pair \(((x_1, x_2), (y_1, y_2))\) of elements of
        \(\R^2\) to \(|x_1 y_1| + |x_2 y_2|\) is not an inner product on \(\R^2\).
        \item Show that the function taking an ordered pair \(((x_1, x_2, x_3), (y_1, y_2, y_3))\) of
        elements of \(\R^3\) to \(x_1y_1 + x_3y_3\) is not an inner product on \(\R^3\).
    \end{enumerate}
\end{problem}

\begin{proof}
(a) Consider \(x = (2, -2)\) and \(y = (-2, 2)\) and \(z = (1, 1)\). Then \(\langle x,z \rangle
= \langle y,z \rangle = 4\), but \(\langle x + y,z \rangle = 0\).

(b) We have \(\langle (0, 1, 0),(0, 1, 0) \rangle = 0\) but the element is nonzero.
\end{proof}

\begin{problem}{4}
    Suppose \(T \in \Lc(V)\) is such that \(\norm{Tv} \leq \norm{v}\) for every \(v \in V\). Prove that
    \(T - \sqrt{2}I\) is injective.
\end{problem}

\begin{proof}
Suppose for contradiction that \( T - \sqrt{2} I \) is not injective and therefore \(\sqrt{2}\) is an eigenvalue
of \(T\), so \(Tv = \sqrt{2}v\) for some nonzero \(v\). Taking the norm yields that
\[\norm{Tv} = \sqrt{2} \norm{v}\]
which violates the assumption that \(\norm{Tv} \leq \norm{v}\).
\end{proof}

\begin{problem}{5}
    Suppose \(V\) is a real inner product space.
    \begin{enumerate}[label=(\alph*)]
        \item Show that \(\langle u+v,u-v \rangle = \norm{u}^2 - \norm{v}^2\) for every \(u, v \in V\).
        \item Show that if \(u, v \in V\) have the same norm, then \(u + v\) is orthogonal to \(u - v\).
        \item Use (b) to show that the diagonals of a rhombus are perpendicular to each other.
    \end{enumerate}
\end{problem}

\begin{proof}
(a) We have that
\begin{align*}
    \langle u + v,u - v \rangle
    &= \langle u,u \rangle - \langle v,v \rangle - \langle u,v \rangle + \langle v,u \rangle \\
    &= \norm{u}^2 - \norm{v}^2
\end{align*}

(b) We know \(\norm{u} = \norm{v}\), then
\begin{align*}
    \langle u+v,u-v \rangle
    &= \norm{u}^2 - \norm{v}^2 = 0
\end{align*}
which shows that they are orthogonal.

(c) omitted.
\end{proof}

\begin{problem}{6}
    Suppose \(u, v \in V\). Prove that \(\langle u,v \rangle = 0 \Longleftrightarrow \norm{u} \leq \norm{u
    + av}\) for all \(a \in \F\).
\end{problem}

\begin{proof}
\(\Rightarrow\) Given \(\langle u,v \rangle = 0\), then
\begin{align*}
    \norm{u + av}^2
    &= \langle u + av, u + av \rangle \\
    &= \norm{u}^2 + \overline{a}\langle u,v \rangle + |a|^2 \langle v,v \rangle \\
    &= \norm{u}^2 + |a|^2 \langle v,v \rangle \\
    &\geq \norm{u}^2
\end{align*}

\(\Leftarrow\) If \(v = 0\), then it's trivial. Consider \(v \neq 0\). Let \(a = \frac{\langle u,v \rangle}{\norm{v}^2}\).
Then we have that
\begin{align*}
    \norm{u - \frac{\langle u,v \rangle}{\norm{v}^2}v}^2
    &= \left\langle u - \frac{\langle u,v \rangle}{\norm{v}^2}v, u - \frac{\langle u,v \rangle}{\norm{v}^2}v \right\rangle \\
    &= \norm{u}^2 - \overline{\frac{\langle u,v \rangle}{\norm{v}^2}} \langle u, v \rangle
    - \frac{\langle u,v \rangle}{\norm{v}^2} \langle v,u \rangle +
    \left\lvert \frac{\langle u,v \rangle}{\norm{v}^2} \right\rvert^2 \norm{v}^2 \\
    &= \norm{u}^2 - 2\frac{|\langle u,v \rangle|^2}{\norm{v}^2} + \frac{|\langle u,v \rangle|^2}{\norm{v}^2} \\
    &= \norm{u}^2 - \frac{|\langle u,v \rangle|^2}{\norm{v}^2} \geq \norm{u}^2
\end{align*}

This implies that
\[\frac{|\langle u,v \rangle|^2}{\norm{v}^2} = 0\]
Since \(v \neq 0\), \(\langle u,v \rangle = 0\).
\end{proof}

\begin{problem}{7}
    Suppose \(u, v \in V\). Prove that \(\norm{au + bv} = \norm{bu + av}\) for all
    \(a, b \in \R\) if and only if \(\norm{u} = \norm{v}\).
\end{problem}

\begin{proof}
Notice that
\begin{align*}
    \norm{au + bv}^2
    &= \langle au + bv, au+bv \rangle \\
    &= |a|^2 \norm{u}^2 + a\overline{b}\langle u,v \rangle
    + b\overline{a}\langle v,u \rangle + |b|^2 \norm{v}^2  \\
    &=|a|^2 \norm{u}^2 + |b|^2 \norm{v}^2 + ab(\langle u,v \rangle + \langle v,u \rangle)
\end{align*}

At the same time we have
\[\norm{bu + av}^2 = |b|^2 \norm{u}^2 + |a|^2 \norm{v}^2 + ab(\langle u,v \rangle + \langle v,u \rangle)\]
Then this means \(\norm{au + bv} = \norm{bu + av}\) for all
\(a, b \in \R\) iff \(|a|^2 \norm{u}^2 + |b|^2 \norm{v}^2 = |b|^2 \norm{u}^2 + |a|^2 \norm{v}^2 \)
for all \(a, b \in \R\) iff \(\norm{u} = \norm{v}\).
\end{proof}

\begin{problem}{8}
    Suppose \(a,b,c,x,y \in \R\) and \(a^2 + b^2 + c^2 + x^2 + y^2 \leq 1\). Prove that
    \(a + b + c + 4x + 9y \leq 10.\)
\end{problem}

\begin{proof}
Let
\[u = (a, b, c, x, y) \ \ \ v = (1, 1, 1, 4, 9)\]
and consider the standard real euclidean inner product. Then we can apply the Cauchy-Schwarz:
\[|\langle u,v \rangle|^2 = \left(\sum_{i=1}^{5}u_i v_i \right)^2 \leq \left(\sum_{i=1}^{5} u_i\right)^2 \left(\sum_{i=1}^{5}v_i\right)^2 = \norm{u}^2 \norm{v}^2\]

Expanding this gives that
\[(a + b + c + 4x + 9y)^2 \leq (a^2+b^2+c^2+x^2+y^2)(1+1+1+16+81) \leq 100\]
Therefore, we have that
\[a + b + c + 4x + 9y \leq 10\]
\end{proof}

\begin{problem}{9}
    Suppose \(u, v \in V\) and \(\norm{u} = \norm{v} = 1\) and \(\langle u,v \rangle = 1\). Prove
    that \(u = v\).
\end{problem}

\begin{proof}
Suppose for contradiction that \(u \neq v\), then \(u - v \neq 0\). Then
\[\norm{u - v}^2 = \langle u-v,u-v \rangle = \norm{u}^2 + \norm{v}^2 - \langle u,v \rangle
- \langle v,u \rangle= 2 - 2 = 0\]
forming a contradiction. Therefore, \(u = v\).
\end{proof}

\begin{problem}{10}
    Suppose \(u, v \in V\) and \(\norm{u} \leq 1\) and \(\norm{v} \leq 1\). Prove that
    \[\sqrt{1 - \norm{u}^2} \sqrt{1 - \norm{v}^2} \leq 1 - |\langle u,v \rangle|.\]
\end{problem}

\begin{proof}
Notice that by the Cauchy-Schwarz \( |\langle u,v \rangle| \leq \norm{u}\norm{v} = 1\). Hence, we have that
\[1 - \norm{u} \norm{v} \leq 1 - |\langle u,v \rangle|\]
So now it suffices to show that
\[(1 - \norm{u}^2)(1 - \norm{v}^2) \leq (1 - \norm{u}^2 \norm{v}^2)\]
This is not hard to see, as r.h.s - l.h.s = \((\norm{u} - \norm{v})^2 \geq 0\).
\end{proof}

\begin{problem}{12}
    Suppose \(a,b,c,d\) are positive numbers.
    \begin{enumerate}[label=(\alph*)]
        \item Prove that \((a+b+c+d)(\frac{1}{a} + \frac{1}{b} + \frac{1}{c} + \frac{1}{d}) \geq 16\).
        \item For which positive numbers \(a,b,c,d\) is the inequality above an equality?
    \end{enumerate}
\end{problem}

\begin{proof}
(a) Let \(u = (\sqrt{a},\sqrt{b},\sqrt{c},\sqrt{d})\) and \(v =
(\frac{1}{\sqrt{a}}, \frac{1}{\sqrt{b}}, \frac{1}{\sqrt{c}}, \frac{1}{\sqrt{d}})\). Then applying the
Cauchy-Schwarz yields that
\begin{align*}
    \langle u,v \rangle^2 = 16 \leq = (a+b+c+d)(\frac{1}{a}+\frac{1}{b}+\frac{1}{c}+\frac{1}{d})= \norm{u}^2 \norm{v}^2
\end{align*}

(b) By the Cauchy-Schwarz, this is an equality iff \(u = cv\), i.e. \((\sqrt{a}, \sqrt{b}, \sqrt{c}, \sqrt{d})
= c (\frac{1}{\sqrt{a}}, \frac{1}{\sqrt{b}}, \frac{1}{\sqrt{c}}, \frac{1}{\sqrt{d}})\), which holds
if \(a=b=c=d\).
\end{proof}

\begin{problem}{13}
    Show that the square of an average is less than or equal to the average of the squares. More
    precisely, show that if \(a_1, \ldots, a_n \in \R\), then the square of the average of
    \(a_1, \ldots, a_n\) is less than or equal to the average of \(a_1^2, \ldots, a_n^2\).
\end{problem}

\begin{proof}
We try to prove
\[\left(\frac{1}{n} \sum_{i=1}^{n} a_i \right)^2 \leq \frac{1}{n} \sum_{i=1}^{n}a_i^2\]

Take \(u = (a_1, \ldots, a_n)\) and \(v = (\frac{1}{n}, \ldots, \frac{1}{n})\). Then applying the Cauchy-Schwarz yields
that
\begin{align*}
    \langle u,v \rangle^2
    = \left(\frac{1}{n} \sum_{i=1}^{n} a-i \right)^2
    \leq \left(\sum_{i=1}^{n} a_i^2 \right)\frac{1}{n} = \norm{u}^2 \norm{v}^2
\end{align*}
\end{proof}

\begin{problem}{15}
    Suppose \(u, v\) are nonzero vectors in \(\R^2\). Prove that
    \[\langle u,v \rangle = \norm{u} \norm{v} \cos \theta ,\]
    where \(\theta\) is the angle between \(u\) and \(v\).
\end{problem}

\begin{proof}
By law of cosines, we have that
\[\norm{u - v}^2 = \norm{u}^2 + \norm{v}^2 - 2 \norm{u}\norm{v}\cos \theta\]
This means that
\begin{align*}
    2 \norm{u} \norm{v}\cos \theta
    &= \norm{u}^2 + \norm{v}^2 - \norm{u - v}^2 \\
    &= \norm{u}^2 + \norm{v}^2 + 2 \langle u,v \rangle - \norm{u}^2 - \norm{v}^2 \\
    &= 2 \langle u,v \rangle
\end{align*}
\end{proof}

\begin{problem}{17}
    Prove that
    \[\left(\sum_{k=1}^{n} a_k b_k \right)^2 \leq \left(\sum_{k=1}^{n} k a_k^2\right) \left(\sum_{k=1}^{n} \frac{b_k^2}{k}\right)\]
\end{problem}

\begin{proof}
Consider \(u = (a_1, \sqrt{2}a_2, \ldots, \sqrt{n}a_n)\) and
\(v = \left(b_1, \frac{b_2}{\sqrt{2}}, \ldots, \frac{b_n}{\sqrt{n}}\right)\). Applying the Cauchy-Schwarz solves the problem.
\end{proof}

% \begin{problem}{18}
%     \begin{enumerate}[label=(\alph*)]
%         \item Suppose \(f \colon [1, \infty) \to [0, \infty)\) is continuous. Show that
%         \[(\int_{1}^{\infty} f)^2 \leq \int_{1}^\infty x^2 (f(x))^2 dx.\]

%         \item For which continuous functiosn \(f \colon [1, \infty) \to [0, \infty)\) is the inequality
%         in (a) an equality with both sides finite?
%     \end{enumerate}
% \end{problem}

% \begin{proof}
% % (a) Consider \(g(x) = f(x) , h(x) = 1\), then applying the Cauchy-Schwarz gives the desired result.
% % (b) \(f(x)\) is a constant function.
% \end{proof}

\begin{problem}{19}
    Suppose \(v_1, \ldots, v_n\) is a basis of \(V\) and \(T \in \Lc(V)\). Prove that
    if \(\lambda\) is an eigenvalue of \(T\), then
    \[|\lambda|^2 \leq \sum_{j=1}^{n} \sum_{k=1}^{n} |\Mc(T)_{j,k}|^2,\]
    where \(\Mc(T)_{j,k}\) denotes the entry in row \(j\), column \(k\) of the matrix of \(T\)
    wrt. the basis \(v_1, \ldots, v_n\).
\end{problem}

\begin{proof}
% Let \(v = \sum_{i=1}^{n} a_i v_i \in V\). Then we know that
% \[Tv = \sum_{i=1}^{n}a_i Tv_i = \sum_{i=1}^{n}a_i \sum_{j=1}^{n} \Mc(T)_{j,i} v_j = \lambda v = \lambda
% \sum_{i=1}^{n} a_i v_i\]
% This means that
% \[\sum_{j=1}^{n} (\sum_{i=1}^{n} a_i \Mc(T)_{j,i}) v_j =  \sum_{i=1}^{n} (\lambda a_i) v_i\]

% For each coefficient of \(v_j\) this means that
% \[\lambda a_j = \sum_{i=1}^{n} a_i \Mc(T)_{j, i}\]

\begin{align*}
    |\lambda|^2 \norm{v}^2 = || \Mc(T) v ||^2 \leq || \Mc(T) ||_F^2 \norm{v}^2
\end{align*}

for nonzero eigenvector \(v\). Then expanding the Frobenius norm of \(\Mc(T)\) gets the desired inequality.
\end{proof}

\begin{problem}{20}
    Prove the \textbf{reverse triangular inequality}: if \(u, v \in V\), then \(| \norm{u} - \norm{v}|
    \leq \norm{u - v}\).
\end{problem}

\begin{proof}
\begin{align*}
    \norm{u - v}^2
    &= \langle u - v, u - v \rangle \\
    &= \norm{u}^2 + \norm{v}^2 - (\langle u,v \rangle + \langle v,u \rangle) \\
    &\geq \norm{u}^2 + \norm{v}^2 - 2 \norm{u} \norm{v} \\
    &= (\norm{u} - \norm{v})^2
\end{align*}
Taking off the square yields the expected solution.
\end{proof}

\begin{problem}{21}
    Suppose \(u, v \in V\) such that
    \[\norm{u} = 3, \ \ \norm{u+v} = 4, \ \ \norm{u - v} = 6.\]
    What number does \(\norm{v}\) equal?
\end{problem}

\begin{proof}
We know that
\begin{align*}
    \norm{v} &\geq \norm{u+v} - \norm{u} = 1\\
    \norm{v}^2 &= ( \norm{u+v}^2 + \norm{u - v}^2)/2 - \norm{u}^2 = (16+36)/2 - 9 =  17
\end{align*}
So \(\norm{v} = \sqrt{17}\).
\end{proof}

\begin{problem}{22}
    Show that if \(u, v \in V\), then
    \[\norm{u +v} \norm{u - v} \leq \norm{u}^2 + \norm{v}^2.\]
\end{problem}

\begin{proof}
Let \(a = \norm{u+v}, b = \norm{u - v}\), then we know that
\[a^2 + b^2 = 2(\norm{u}^2 + \norm{v}^2)\]

We have that
\[(a - b)^2 \geq 0\]
Expanding it gives that
\begin{align*}
    (a-b)^2 = (a^2 + b^2) - 2ab \geq 0
\end{align*}
equivalently,
\[\norm{u}^2 + \norm{v}^2 \geq \norm{u+v} \norm{u - v}\]
\end{proof}

\begin{problem}{23}
    Suppose \(v_1, \ldots, v_m \in V\) are such that \(\norm{v_k} \leq 1\) for each
    \(k = 1, \ldots, m\). Show that there exists \(a_1, \ldots, a_m \in \{1, -1\}\)
    such that
    \[\norm{a_1 v_1 + \cdots + a_m v_m} \leq \sqrt{m}.\]
\end{problem}

\begin{proof}
We consider a probabilistic approach: Let \(a_1, \ldots, a_m\) be the iid Rademacher variables
such with \(a_i = 1\) w.p. \(1/2\) and \(a_i = 0\) w.p. \(1/2\). Then we can define a random
vector
\[X = \sum_{i=1}^{m}a_i v_i\]
and we can compute the expected value
\[\E \left[\norm{X}^2 \right]
= \E \left[ \norm{\sum_{i=1}^{m} a_i v_i} ^2 \right]
= \E \left[ \left(\sum_{i=1}^{m} a_i v_i \sum_{j=1}^{m}a_j v_j \right) \right]
=  \sum_{i=1}^{m} \sum_{j=1}^{m} (v_i \cdot v_j) \E [a_i a_j]\]
Note that here \(\E[a_i a_j] = \delta_{ij}\) and that
\[\E \left[\norm{X}^2 \right]  = \sum_{k=1}^{m} \E[a_k^2](v_k \cdot v_k) = \sum_{k=1}^{m} \norm{v_k}^2 \leq m\]
which gives that
\[\E \left[\norm{X} \right]  \leq \sqrt{m}\]
and shows the existence proof.
\end{proof}

\begin{problem}{25}
    Suppose \(p > 0\). Prove that there is an inner product on \(\R^2\) such that the associated norm
    is given by
    \[\norm{(x, y)} = (|x|^p + |y|^{p})^{1/p}\]
    for all \((x,y) \in \R^2\) if and only if \(p = 2\).
\end{problem}

\begin{proof}
\(\Leftarrow\) Given \(p = 2\), the natural euclidean dot product induces a well-defined norm, e.g.
\(\norm{(x,y)} = (x^2 + y^2)^{1/2}\).

\(\Rightarrow\) Note that the parallelogram equalities need to hold. Thus pick \(u = (1,0), v = (0, 1)\),
and then
\[\norm{u + v}^2 + \norm{u - v}^2 = 2\cdot4^{1/p} \]
and
\[2(\norm{u}^2 + \norm{v}^2) = 4\]
They only equal each other when
\[2 \cdot 4^{1/p} = 4\]
which holds only if \(p = 2\).
\end{proof}

\begin{problem}{26}
    Suppose \(V\) is a real inner product space. Prove that
    \[\langle u,v \rangle = \frac{\norm{u + v}^2 - \norm{u - v}^2}{4}\]
    for all \(u, v \in V\).
\end{problem}

\begin{proof}
\begin{align*}
    \norm{u + v}^2 - \norm{u - v}^2
    &= \langle u+v,u+v \rangle - \langle u-v,u-v \rangle \\
    &= (\norm{u}^2 + 2 \langle u,v \rangle + \norm{v}^2) - (\norm{u}^2 - 2 \langle u,v \rangle + \norm{v}^2) \\
    &= 4 \langle u,v \rangle
\end{align*}
\end{proof}

\begin{problem}{29}
    Suppose \(V_1, \ldots, V_m\) are inner product spaces. Show that the equation
    \[\langle (u_1, \ldots, u_m), (v_1, \ldots, v_m) \rangle = \langle u_1,v_1 \rangle
    + \cdots + \langle u_m,v_m \rangle\]
    defines an inner product on \(V_1 \times \cdots \times V_m\).
\end{problem}

\begin{proof}
We check this by definition. Let \(u, v, w \in V_1 \times \cdots \times V_m\).

\textbf{positivity}: \(\langle v,v \rangle
= \langle v_1,v_1 \rangle + \cdots + \langle v_m,v_m \rangle \geq 0 \)
as each of them \(\geq 0\).

\textbf{definiteness}: Suppose that \(\langle v,v \rangle
= \langle v_1,v_1 \rangle + \cdots + \langle v_m,v_m \rangle = 0 \). Then as each of the individual
element \(\geq 0\), the only solution is \(v = 0\). Conversely, if \(v = 0\), then \(\langle v,v \rangle = 0\).

\textbf{additivity in first slot}: \(\langle u+v,w \rangle
= \langle u_1 + v_1, w_1 \rangle + \cdots + \langle u_m + v_m, w_m \rangle
= (\langle u_1,w_1 \rangle + \cdots + \langle u_m,w_m \rangle)
+ (\langle v_1,w_1 \rangle + \cdots + \langle v_m,w_m \rangle)
= \langle u,w \rangle + \langle v,w \rangle\)

\textbf{homogeneity in first slot}: follows similarly as above.

\textbf{conjugate symmetry}: follows similarly as above.
\end{proof}

\begin{problem}{31}
    Suppose \(u, v, w \in V\). Prove that
    \[\norm{w - \frac{1}{2}(u+v)}^2 = \frac{\norm{w - u}^2 + \norm{w - v}^2}{2} - \frac{\norm{u-v}^2}{4}.\]
\end{problem}

\begin{proof}
Let \(w - u = a\) and \(w - v = b\), then we have
\begin{align*}
    \text{l.h.s}
    &= \norm{ a/2  + b/2  }^2 \\
    &=  2(\norm{a/2}^2 + \norm{b/2}^2) - \norm{a/2 - b/2}^2 \\
    &= \frac{\norm{a}^2 + \norm{b}^2}{2} - \frac{a - b}{4} = \text{r.h.s}
\end{align*}
Substituting \(a\) and \(b\) gets the desired result.
\end{proof}

\begin{problem}{32}
    SUppose that \(E\) is a subset of \(V\) with the property that \(u, v \in E\) implies
    \(\frac{1}{2}(u+v) \in E\). Let \(w \in V\). Show that there is at most one point in \(E\)
    that is cloest to \(w\). In other words, show that there is at most one \(u \in E\) such that
    \[\norm{w - u} \leq \norm{w - x}\]
    for all \(x \in E\).
\end{problem}

\begin{proof}
Suppose for contradiction that there is another \(v \in E, v \neq u\) such that
\[\norm{w - v} \leq \norm{w - x}\]
for all \(x \in E\). Then we have that
\[\norm{w - \frac{1}{2}(u+v)} = \frac{\norm{w - u}^2 + \norm{w - v}^2}{2} - \frac{\norm{u-v}^2}{4}\]
by problem 31. Notice that
\[\frac{\norm{w - u}^2 + \norm{w - v}^2}{2} - \frac{\norm{u-v}^2}{4} \leq \norm{w - x} -
\frac{\norm{u-v}^2}{4} \leq \norm{w - x}\]
for all \(x \in E\), reaching a contradiction (\(u = v\)).
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 6B %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{6B: Orthonormal Bases}
\addcontentsline{toc}{section}{6B: Orthonormal Bases}

\begin{definition}[orthonormal]
    A list of vectors is called \textbf{orthonormal} if each vector in the list has norm 1 and
    is orthogonal to all the other vectors in the list.
\end{definition}

\begin{corollary}
    Suppose \(e_1, \ldots, e_m\) is an orthonormal list of vectors in \(V\). Then
    \[\norm{a_1 e_1 + \cdots + a_m e_m}^2 = |a_1|^2 + \cdots + |a_m|^2\]
    for all \(a_1, \ldots, a_m \in \F\).
\end{corollary}

\begin{corollary}
    Every orthonormal list of vectors is linearly independent.
\end{corollary}

\begin{thm}[Bessel's inequality]
    Suppose \(e_1, \ldots, e_m\) is an orthonormal list of vectors in \(V\). If \(v \in V\) then
    \[ |\langle v,e_1 \rangle|^2 + \cdots + |\langle v,e_m \rangle|^2 \leq \norm{v}^2\]
\end{thm}

\begin{definition}[orthonormal basis]
    An \textbf{orthonormal basis} of \(V\) is an orthogonal list of vectors in \(V\) that is
    also a basis of \(V\).
\end{definition}

\begin{corollary}
    Suppose \(V\) is finite-dimensional. Then every orthonormal list of vectors in \(V\)
    of length \(\dim V\) is an orthonormal basis of \(V\).
\end{corollary}

\begin{remark}
    Usually we write \(v = \sum_{i=1}^{n} a_i v_i\), but with orthonormal basis we can just
    take \(a_k = \langle v,e_k \rangle\).
\end{remark}

\begin{lemma}[writing a vector as a linear combination of an orthonormal basis]
    Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\) and \(u, v \in V\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \(v = \langle v,e_1 \rangle e_1 + \langle v,e_n \rangle e_n\),
        \item \(\norm{v}^2 = |\langle v,e_1 \rangle|^2 + \cdots + |\langle v,e_n \rangle|^2\),
        \item \(\langle u,v \rangle = \langle u,e_1 \rangle \overline{\langle v, e_1 \rangle} + \cdots + \langle u,e_n \rangle
        \overline{\langle v,e_n \rangle}\).
    \end{enumerate}
\end{lemma}

\begin{thm}[Gram-Schmidt procedure]
    Suppose \(v_1, \ldots, v_m\) is a linearly independent list of vectors in \(V\). Let \(f_1 = v_1\).
    For \(k = 2, \ldots, m\), define \(f_k\) inductively by
    \[f_k = v_k - \frac{\langle v_k,f_1 \rangle}{\norm{f_1}^2}f_1 - \cdots - \frac{\langle v_k, f_{k-1} \rangle}{\norm{f_{k-1}}^2}f_{k-1}.\]
    For each \(k = 1,\ldots, m\), let \(e_k = \frac{f_k}{\norm{f_k}}\). Then \(e_1, \ldots, e_m\) is an
    orthonormal list of vectors in \(V\) such that
    \[\text{span}(v_1, \ldots, v_k) = \text{span}(e_1, \ldots, e_k)\]
    for each \(k = 1, \ldots, m\).
\end{thm}

\begin{corollary}
    Every finite-dimensional inner product space has an orthornormal basis.
\end{corollary}

\begin{corollary}
    Suppose \(V\) is finite-dimensional. Then every orthonormal list of vectors in \(V\) can
    be extended to an orthonormal basis of \(V\).
\end{corollary}

\begin{lemma}[upper-triangular matrix with respect to some orthonormal basis]
    Suppose \(V\) is finite-dimensional and \(T \in \Lc(V)\). Then \(T\) has an upper-triangular matrix
    with respect to some orthonormal basis of \(V\) if and only if the minimal polynomial of \(T\) equals
    \((z - \lambda_1) \cdots (z - \lambda_m)\) for some \(\lambda_1, \ldots, \lambda_m \in \F\).
\end{lemma}

\begin{thm}[Schur's theorem]
    Every operator on a finite-dimensional complex inner product space has an upper-triangular matrix
    with respect to some orthonormal basis.
\end{thm}

\begin{thm}[Riesz representation theorem]
    Suppose \(V\) is finite-dimensional and \(\varphi\) is a linear functional on \(V\). Then there
    is a unique vector \(v \in V\) such that
    \[\varphi(u) = \langle u,v \rangle\]
    for every \(u \in V\).
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 6B PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{6B Problem Sets}

\begin{problem}{1}
    Suppose \(e_1, \ldots, e_m\) is a list of vectors in \(V\) such that
    \[\norm{a_1 e_1 + \cdots + a_m e_m}^2 = |a_1|^2 + \cdots + |a_m|^2\]
    for all \(a_1, \ldots, a_m \in \F\). Show that \(e_1, \ldots, e_m\) is an orthonormal list.
\end{problem}

\begin{proof}
First, note that if we choose \( a_j = 1 \) and \( a_k = 0, k \ne j \), we have
\( \langle e_j, e_j \rangle = \norm{e_j}^2 = 1 \).

Further, for all \(a_1, \ldots, a_m \in \F\)
\begin{align*}
    \sum_{i=1}^{m} |a_i|^2
    = \norm{\sum_{i=1}^{m} a_i e_i}^2
    = \left\langle \sum_{i=1}^{m} a_i e_i , \sum_{i=1}^{m} a_i e_i \right\rangle
    = \sum_{i=1}^{m} \sum_{j=1}^{m} a_i \overline{a_j} \langle e_i, e_j \rangle \\
    = \sum_{i=1}^{m} \lvert a_i \rvert^2 \langle e_i, e_i \rangle
    + \sum_{i \ne j}^{m} a_i \overline{a_j} \langle e_i, e_j \rangle
    = \sum_{i=1}^{m} |a_i|^2
    + \sum_{i \ne j}^{m} a_i \overline{a_j} \langle e_i, e_j \rangle.
\end{align*}

That is, for all \(a_1, \ldots, a_m \in \F\)
\begin{align*}
    \sum_{i \ne j}^{m} a_i \overline{a_j} \langle e_i, e_j \rangle = 0.
\end{align*}

Choosing \( a_i=1, a_j =1 \) and \( a_k=0 \) for \( i \ne j, k\), we get
\( \langle e_j, e_k \rangle = 0 \).
Thus, we have \(  \langle e_i, e_j \rangle = \delta_{ij} \) which means that
\( e_1, \ldots, e_m \) is orthonormal.
\end{proof}

\begin{problem}{3}
    Suppose \(e_1, \ldots, e_m\) is an orthonormal list in \(V\) and \(v \in V\). Prove that
    \[\norm{v}^2 = |\langle v,e_1 \rangle|^2 + \cdots + |\langle v,e_m \rangle|^2
    \Longleftrightarrow v \in \text{span}(e_1, \ldots, e_m)\]
\end{problem}

\begin{proof}
\(\Rightarrow\) We can decompose \(v\) into two parts, one is \(v_{proj} = \sum_{i=1}^{m}
\langle v,e_i \rangle e_i\), which is the orthogonal projection of \(v\) onto the subspace
spanned by \(e_1, \ldots, e_m\). We claim that \(v - v_{proj}\) is orthogonal to \(v_{proj}\).
This can be seen as
\begin{align*}
    \langle v_{proj}, v - v_{proj} \rangle
    &= \left\langle \sum_{i=1}^{m} \langle v,e_i \rangle e_i, v - \sum_{j=1}^{m} \langle v,e_j \rangle e_j \right\rangle \\
    &= \sum_{i=1}^{m} |\langle v,e_i \rangle|^2 - \sum_{i=1}^{m} |\langle v,e_i \rangle|^2 = 0
\end{align*}

Then by Pythagorean theorem we have
\[\norm{v}^2 = \norm{v_{proj}}^2 + \norm{v - v_{proj}}^2 \]
where \(\norm{v}^2 = \norm{v_{proj}^2}\) and thus \(v = v_{proj}\). Equivalently, \(v \in \text{span}
(e_1, \ldots, e_m)\).

\(\Leftarrow\) This means that \(v = \sum_{i=1}^{m} a_i e_i\). However, we know that
\(a_i = \langle v,e_i \rangle\), so \(\norm{v}^2 = \sum_{j=1}^{m} |\langle v,e_j \rangle|^2\) by
repeatedly applying the Pythagorean theorem.
\end{proof}

\begin{problem}{4}
    Suppose \(n\) is a positive integer. Prove that
    \[\frac{1}{\sqrt{2\pi}}, \frac{\cos x}{\sqrt{\pi}}, \frac{\cos 2x}{\sqrt{\pi}},
    \cdots, \frac{\cos n x}{\sqrt{\pi}}, \frac{\sin x}{\sqrt{\pi}}, \frac{\sin 2x}{\sqrt{\pi}}, \cdots
    \frac{\sin n x}{\sqrt{\pi}}\]
    is an orthonormal list of vectors in \(C[-\pi, \pi]\), the vector space of continuous real-valued functions on
    \([-\pi, \pi]\) with inner product
    \[\langle f,g \rangle = \int_{-\pi}^{\pi} fg.\]
\end{problem}

\begin{proof}
First, we show each of the element has norm 1.
\begin{align*}
    \norm{\frac{1}{\sqrt{2\pi}}}
    = \sqrt{ \int_{-\pi }^{\pi} \frac{1}{2 \pi} dx } = 1 \\
    \norm{\frac{\cos nx}{\sqrt{\pi}}}
    = \sqrt{\int_{-\pi }^{\pi} \frac{\cos^2 nx}{\pi} dx}
    = \sqrt{\frac{1}{\pi} [  \frac{x}{2} + \frac{\sin (2nx)}{4n}  ]_{-\pi}^\pi}
    =  1 \\
    \norm{\frac{\sin nx}{\sqrt{\pi}}}
    = \sqrt{\int_{-\pi }^{\pi} \frac{\sin^2 nx}{\pi} dx} =
    \sqrt{ \frac{1}{\pi} [  \frac{x}{2} - \frac{\cos (2nx)}{4n}  ]_{-\pi}^\pi  } = 1
\end{align*}

Next, we show that each element is orthogonal to each other, there are many different cases, we begin
examine here:
\begin{align*}
    \langle \frac{1}{\sqrt{2\pi}}, \frac{\cos nx}{\sqrt{\pi}} \rangle
    =  \frac{1}{\sqrt{2}\pi} \int_{-\pi}^{\pi} \cos nx dx = \frac{1}{\sqrt{2}\pi} [\frac{\sin nx}{n}]_{-\pi}^\pi
    =  0 \\
    \langle \frac{1}{\sqrt{2\pi}}, \frac{\sin nx}{\sqrt{\pi}} \rangle
    = \frac{1}{\sqrt{2} \pi} \int_{-\pi }^{\pi} \sin nx dx = \frac{1}{\sqrt{2}\pi} [-\frac{\cos nx}{n}]_{-\pi}^\pi
    = 0
\end{align*}

Similarly, one can derive between every different pairs of element, their inner product is 0 for different
index. The derivation is omitted.
\end{proof}

\begin{problem}{6}
    Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\).
    \begin{enumerate}[label=(\alph*)]
        \item Prove that if \(v_1, \ldots, v_n\) are vectors in \(V\) such that
        \[\norm{e_k - v_k} < \frac{1}{\sqrt{n}}\]
        for each \(k\), then \(v_1, \ldots, v_n\) is a basis of \(V\).
        \item Show that there exist \(v_1, \ldots, v_n \in V\) such that
        \[\norm{e_k - v_k} \leq \frac{1}{\sqrt{n}}\]
        for each \(k\), but \(v_1, \ldots, v_n\) is not linearly independent.
    \end{enumerate}
\end{problem}

\begin{proof}
(a) Suppose for contradiction that \(v_1, \ldots, v_n\) is not a basis and thus linearly dependent.
Then there exists scalars \(a_1, \ldots, a_n \in \F\) not all zero such that \(\sum_{i=1}^{n}a_iv_i = 0\).
Then we have that
\[\sum_{i=1}^{n} a_i (v_i - e_i) +  \sum_{i=1}^{n} a_i e_i = 0\]
which means that
\[\norm{\sum_{i=1}^{n} a_i (v_i - e_i)} = \norm{\sum_{i=1}^{n} a_i e_i}\]

Note that
\[\norm{\sum_{i=1}^{n} a_i (v_i - e_i)} \leq \sum_{i=1}^{n} \norm{a_i (v_i - e_i)}
= \sum_{i=1}^{n} |a_i| \norm{v_i - e_i} < \sum_{i=1}^{n} \frac{|a_i|}{\sqrt{n}} \leq \left(\sum_{i=1}^{n} |a_i|^2\right)^{1/2}\]

where the last inequality is shown by the Cauchy-Schwarz. This reaches a contradiction, as
\[\norm{\sum_{i=1}^{n} a_i e_i} = \left(\sum_{i=1}^{n} |a_i|^2 \right)^{1/2} < \left(\sum_{i=1}^{n} |a_i|^2 \right)^{1/2}\]

(b) Suppose \(v_1 = e_1 + \frac{1}{\sqrt{n}}e_2\) and \(v_j = e_j\) for \(2 \leq j \leq n\). Hence we have
\[\norm{e_1 - v_1} = \norm{\frac{1}{\sqrt{n}} e_2} = \frac{1}{\sqrt{n}}\]
where other conditions hold trivially. However, we can clearly tell that \(v_1, \ldots, v_n\) is not
linearly independent.
\end{proof}

\begin{problem}{9}
    Suppose \(e_1, \ldots, e_m\) is the result of applying the Gram-Schmidt procedure to a linearly
    independent list \(v_1, \ldots, v_m\) in \(V\). Prove that \(\langle v_k,e_k \rangle > 0\) for
    each \(k = 1, \ldots, m\).
\end{problem}

\begin{proof}
In the Gram-Schmidt process, we decompose \(v_k\) into \(v_{proj}\) and \(f_k\) where \(v_{proj}\) is
the Orthogonal projection of \(v_k\) onto the \(\text{span}(v_1, \ldots, v_{k-1}) = \text{span}(e_1, \ldots, e_{k-1})\).
To show \(\langle v_k,e_k \rangle > 0\), it's equivalent to show \(\langle v_k, f_k \rangle > 0\), which
naturally holds as
\[\langle v_k,f_k \rangle = \langle f_k + v_{proj},f_k \rangle = \langle f_k,f_k \rangle > 0\]
\end{proof}

\begin{problem}{11}
    Find a polynomial \(q \in \Pc_2(\R)\) such that \(p(\frac{1}{2}) = \int_{0}^{1}pq \) for every
    \(p \in \Pc_2(\R)\).
\end{problem}

\begin{proof}
Define \(\varphi \in \Lc(\Pc_2(\R))\) to be \(\varphi(p) = p(\frac{1}{2})\) and consider the inner
product \(\langle p,q \rangle = \int_{0}^{1} pq\). Following the Riesz representation theorem, we
can derive that
\[q =  \overline{\varphi(e_1)}e_1 + \overline{\varphi(e_2)}e_2 + \overline{\varphi(e_3)}e_3\]
where we can consider the orthonormal basis \(\sqrt{\frac{1}{2}}, \sqrt{\frac{3}{2}}x,
\sqrt{\frac{45}{8}}(x^2 - \frac{1}{3})\). Then
\begin{align*}
    q(x) &= \sqrt{\frac{1}{2}} \sqrt{\frac{1}{2}}  + \sqrt{\frac{3}{2}} \frac{1}{2} \sqrt{\frac{3}{2}x}
    + \sqrt{\frac{45}{8}}\left(\frac{1}{4} - \frac{1}{3}\right) \sqrt{\frac{45}{8}} \left(x^2 - \frac{1}{3}\right) \\
    &= \frac{1}{2} + \frac{3}{4}x + \frac{5}{32} - \frac{15}{32}x^2 \\
    &= -\frac{15}{32}x^2 + \frac{3}{4}x + \frac{21}{32}
\end{align*}
\end{proof}

\begin{problem}{13}
    Show that a list \(v_1, \ldots, v_m\) of vectors in \(V\) is linearly dependent if and only if
    the Gram-Schmidt formula produces \(f_k = 0\) for some \(k \in \{1, \ldots, m\}\).
\end{problem}

\begin{proof}
At each step \(k\), the formula aims at decomposes \(v_k = v_{proj} + f_k\), where \(v_{proj}\) is the
orthogonal projection of \(v_k\) onto \(\text{span}(e_1, \ldots, e_{k-1})\). \(f_k = 0\) equivalently
means that \(v_k = v_{proj}\), which means that \(v \in \text{span}(e_1, \ldots, e_{k-1})
= \text{span}(v_1, \ldots, v_{k-1})\) and therefore renders the list to be linearly dependent.
\end{proof}

\begin{problem}{14}
    Suppose \(V\) is a real inner product space and \(v_1, \ldots, v_m\) is a linearly independent list of
    vectors in \(V\). Prove that there exist exactly \(2^m\) orthonormal lists \(e_1, \ldots, e_m\) of
    vectors in \(V\) such that
    \[\text{span}(v_1, \ldots, v_k)  = \text{span}(e_1, \ldots, e_k)\]
    for all \(k \in \{1, \ldots, m\}\).
\end{problem}

\begin{proof}
We prove this statement through induction on \(m\). For base case, consider \(\text{span}(v_1)\) for
nonzero \(v_1 \in V\), there are only two nonzero vectors in \(\text{span}(v_1)\): \(\pm \frac{v_1}{\norm{v_1}}\).
So there are exactly \(2^1 = 1\) orthonormal list of vectors.

For induction, assume that for \(v_1, \ldots, v_{k-1}\) linearly independent list of vectors in \(V\),
there exist exactly \(2^{k-1}\) orthonormal lists \(e_1, \ldots, e_{k-1}\) of vectors in \(V\) such that
\[\Span (v_1, \ldots, v_{k-1}) = \Span (e_1, \ldots, e_{k-1})\]
For \(k\), by the Gram-Schmidt, we have the \(e_k\) such that
\[\Span (v_1, \ldots, v_k) = \Span (e_1, \ldots, e_k)\]
Suppose such choice of \(e_k\) is not unique and there's other \(e_k'\) also satisfies
\[\Span(e_1, \ldots, e_k') = \Span(e_1, \ldots, e_k)\]
which indicates that \(e_k' = \sum_{i=1}^{k} \langle e_k',e_i \rangle e_i = \langle e_k',e_k \rangle e_k\) and that
\[1 = \norm{e_k'} = |\langle e_k', e_k \rangle|\]
so \(e_k' = \pm e_k\) and this gives \(2 * 2^{m-1} = 2^m\) choices of orthonormal lists of vectors.
\end{proof}

\begin{problem}{15}
    Suppose \(\langle \cdot,\cdot \rangle_1\) and \(\langle \cdot,\cdot \rangle_2\) are inner products on
    \(V\) such that \(\langle u,v \rangle_1 = 0\) if and only if \(\langle u,v \rangle_2 = 0\). Prove that
    there is a positive number \(c\) such that \(\langle u,v \rangle_1 = c \langle u,v \rangle_2\) for
    every \(u, v \in V\).
\end{problem}

\begin{proof}
It suffices to prove that \(c = \frac{\langle u,v \rangle_1}{\langle u,v \rangle_2}\) for every
\(u, v \in V\) is a constant number.

First, pick nonzero \(u \in V\). Then we know that \(\langle u,u \rangle_1 > 0, \langle u,u \rangle_2 > 0\).
Pick \(v \in V\) s.t. \(\langle u,v \rangle_1 \neq 0, \langle u,v \rangle_2 \neq 0\) (i.e. they are not
orthogonal). So we have that
\[ \left\langle u - \frac{\langle u,v \rangle_2}{\langle v,v \rangle_2}v, v \right\rangle_1 = 0  =
\left\langle u - \frac{\langle u,v \rangle_2}{\langle v,v \rangle_2}v, v \right\rangle_2 \]
by the orthogonal decomposition of \(u\). This gives that

\[\frac{\langle u,v \rangle_1}{\langle u,v \rangle_2} = \frac{\langle v,v \rangle_1}{\langle v,v \rangle_2}\]

Similarly, we have

\[ \left\langle u , v - \frac{\langle u,v \rangle_2}{\langle v,v \rangle_2}u \right\rangle_1 = 0  =
\left\langle u , v - \frac{\langle u,v \rangle_2}{\langle v,v \rangle_2}u \right\rangle_2 \]

and that
\[\frac{\langle u,v \rangle_1}{\langle u,v \rangle_2} = \frac{\langle u,u \rangle_1}{\langle u,u \rangle_2}
= \frac{\langle v,v \rangle_1}{\langle v,v \rangle_2} = c\]

which yields the desired solution.
\end{proof}

\begin{problem}{16}
    Suppose \(V\) is finite-dimensional. Suppose \(\langle \cdot,\cdot \rangle_1\) and
    \(\langle \cdot,\cdot \rangle_2\) are inner products on \(V\) with corresponding norms
    \(\norm{\cdot}_1\) and \(\norm{\cdot}_2\). Prove that there exists a positive number \(c\)
    such that \(\norm{v}_1 \leq c \norm{v}_2\) for every \(v \in V\).
\end{problem}

\begin{proof}
Let \(e_1, \ldots, e_n\) be an orthonormal basis of \(V\) wrt. \(\langle \cdot,\cdot \rangle_1\)
and \(f_1, \ldots, f_n\) be an orthonormal basis of \(V\) wrt. \(\langle \cdot,\cdot \rangle_2\). Pick
nonzero \( v \in V\). Then there exists \(\varphi \in V'\) such that
\[ \norm{v}_1^2 = \sum_{i=1}^{n} |\langle v,e_i \rangle_1|^2 \leq |\varphi(v)|^2\]
We can proceed with that
\begin{align*}
    \norm{v}_1^2
    &\leq |\varphi(v)|^2  \\
    &= | \langle v, \overline{\varphi(f_1)}f_1 + \cdots + \overline{\varphi(f_n)}f_n \rangle_2  |^2 \\
    &\leq  \norm{\sum_{i=1}^{n} \overline{\varphi(f_i)}f_i}_2^2  \norm{v}_2^2
\end{align*}
which completes the proof.
\end{proof}

\begin{problem}{17}
    Suppose \(\F = \C\) and \(V\) is finite-dimensional. Prove that if \(T\) is an operator on \(V\)
    such that \(1\) is the only eigenvalue of \(T\) and \(\norm{Tv} \leq \norm{v}\) for all \(v \in V\),
    then \(T\) is the identity operator.
\end{problem}

\begin{proof}
    By Schur's theorem, there exists an orthonormal basis \(e_1, \ldots, e_n\) such that the matrix of \(T\) is
    upper-triangular. Then \(1\) is the only component on the diagonal entries. Hence,
    \[\norm{T e_k} \leq \norm{e_k} = 1\]
    Note that \(Te_k = \sum_{i=1}^{k-1}a_i e_i + e_k\) since we know the upper-triangular matrix has diagonal term to be 1.
    This gives that
    \[\norm{\sum_{i=1}^{k-1} a_i e_i + e_k} = \norm{e_k} + \sum_{i=1}^{k-1}|a_i| \norm{e_i} \leq \norm{e_k}\]
    so for each \(e_k\), the off-diagonal entries \(a_i\) are all 0 and thus the matrix of \(T\) is the identity
    matrix, and \(T\) is the identity operator.
\end{proof}

\begin{problem}{18}
    Suppose \(u_1, \ldots, u_m\) is a linearly independent list in \(V\). Show that there exists \(v \in V\)
    such that \(\langle u_k,v \rangle = 1\) for all \(k \in \{1, \ldots, m\}\).
\end{problem}

\begin{proof}
Define \(\varphi \in V'\) s.t. \(\varphi(u_k) = 1\) for all \(k\). By Riesz representation theorem,
there is a unique \(v \in V\) s.t.
\[\varphi(u_k) = \langle u_k, v \rangle = 1\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 6C %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{6C: Orthogonal Complements and Minimization Problems}
\addcontentsline{toc}{section}{6C: Orthogonal Complements and Minimization Problems}

\begin{definition}[orthogonal complement, \(U^{\perp}\)]
    If \(U\) is a subset of \(V\), then the \textbf{orthogonal complement} of \(U\), denoted by
    \(U^\perp\), is the set of all vectors in \(V\) that are orthogonal to every vector in \(U\):
    \[U^\perp = \{v \in V \colon \langle u,v \rangle = 0 \text{ for every } u \in U\}.\]
\end{definition}

\begin{corollary}
    Properties of orthogonal complement: \\
    (a) If \(U\) is a subset of \(V\), then \(U^\perp\) is a subspace of \(V\). \\
    (b) \(\{0\}^\perp = V\). \\
    (c) \(V^\perp = \{0\}\). \\
    (d) If \(U\) is a subset of \(V\), then \(U \cap U^\perp \subseteq \{0\}\). \\
    (e) If \(G\) and \(H\) are subsets of \(V\) and \(G \subseteq H\), then \(H^\perp \subseteq G^\perp\).
\end{corollary}

\begin{corollary}
    Suppose \(U\) is a finite-dimensional subspace of \(V\). Then
    \[V = U \oplus U^\perp\]
    and thus \(\dim U^\perp = \dim V - \dim U\). In addition,
    \[U = (U^\perp)^\perp\]
\end{corollary}

\begin{corollary}
    Suppose \(U\) is a finite-dimensional subspace of \(V\). Then
    \[U^\perp = \{0\} \Longleftrightarrow U = V.\]
\end{corollary}

\begin{definition}[orthogonal projection, \(P_U\)]
    Suppose \(U\) is a finite-dimensional subspace of \(V\). The \textbf{orthogonal projection} of \(V\)
    onto \(U\) is the operator \(P_U \in \Lc(V)\) defined as follows: for each \(v \in V\), write \(v = u + w\),
    where \(u \in U\) and \(w \in U^\perp\). Then let \(P_U v = u\).
\end{definition}

\begin{remark}
    Suppose \(u \in V\) with \(u \neq 0\) and \(U = \Span(u)\). If \(v \in V\), then
    \[v = \frac{\langle v,u \rangle}{\norm{u}^2}u + \left(v - \frac{\langle v,u \rangle}{\norm{u}^2}u \right).\]
    Then this implies that
    \[P_U v = \frac{\langle v,u \rangle}{\norm{u}^2}u\]
\end{remark}

\begin{corollary}[properties of orthogonal projection \(P_U\)]
    Suppose \(U\) is a finite-dimensional subspace of \(V\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \(P_U \in \Lc(V)\);
        \item \(P_U u = u\) for every \(u \in U\);
        \item \(P_U w = 0\) for every \(w \in U^\perp\);
        \item \(\range P_U = U\);
        \item \(\nul P_U = U^\perp\);
        \item \(v - P_Uv \in U^\perp\) for every \(v \in V\);
        \item \(P_U^2 = P_U\);
        \item \(\norm{P_U v} \leq \norm{v}\) for every \(v \in V\);
        \item if \(e_1, \ldots, e_m\) is an orthonormal basis of \(U\) and \(v \in V\), then
        \[P_U v = \langle v,e_1 \rangle e_1 + \cdots + \langle v,e_m \rangle e_m\]
    \end{enumerate}
\end{corollary}

\begin{thm}[Riesz representation theorem, revisited]
    Suppose \(V\) is finite-dimensional. For each \(v \in V\), define \(\varphi_v \in V'\) by
    \[\varphi_v (u) = \langle u,v \rangle\]
    for each \(u \in V\). Then \(v \mapsto \varphi_v\) is a one-to-one function from \(V\) to
    \(V'\).
\end{thm}

\begin{thm}[minimizing distance to a subspace]
    Suppose \(U\) is a finite-dimensional subspace of \(V\), \(v \in V\), and \(u \in U\). Then
    \[\norm{v - P_U v} \leq \norm{v - u}.\]
    Furthermore, the inequality above is an equality if and only if \(u = P_U v\).
\end{thm}

\begin{lemma}[restriction of a linear map to obtain a one-to-one and onto map]
    Suppose \(V\) is finite-dimensional and \(T \in \Lc(V, W)\). Then \(T |_{(\nul T)^\perp}\) is an
    injective map of \((\nul T)^\perp\) onto \(\range T\).
\end{lemma}

\begin{definition}[pseudoinverse, \(T^\dagger\)]
    Suppose that \(V\) is finite-dimensional and \(T \in \Lc(V, W)\). The \textbf{pseudoinverse}
    \(T^\dagger \in \Lc(W, V)\) of \(T\) is the linear map from \(W\) to \(V\) defined by
    \[T^\dagger w = (T|_{(\nul T)^\perp})^{-1} P_{\range T} w\]
    for each \(w \in W\).
\end{definition}

\begin{corollary}[algebraic properties of the pseudoinverse]
    Suppose \(V\) is finite-dimensional and \(T \in \Lc(V, W)\).
    \begin{enumerate}[label=(\alph*)]
        \item If \(T\) is invertible, then \(T^\dagger = T^{-1}\).
        \item \(T T^\dagger = P_{\range T} =\) the orthogonal projection of \(W\) onto range \(T\).
        \item \(T^\dagger T = P_{(\nul T)^\perp} = \) the orthogonal projection of \(V\) onto \((\nul T)^\perp\).
    \end{enumerate}
\end{corollary}

\begin{thm}[pseudoinverse provides best approximate solution or best solution]
    Suppose \(V\) is finite-dimensional, \(T \in \Lc(V, W)\), and \(w \in W\).
    \begin{enumerate}[label=(\alph*)]
        \item If \(v \in V\), then
        \[\norm{T(T^\dagger w) - w} \leq \norm{Tv - w}\]
        with equality if and only if \(v \in T^\dagger w + \nul T\).

        \item If \(v \in T^\dagger w + \nul T\), then
        \[\norm{T^\dagger w} \leq \norm{v},\]
        with equality if and only if \(v = T^\dagger w\).
    \end{enumerate}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 6C PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{6C Problem Sets}

\begin{problem}{1}
    Suppose \(v_1, \ldots, v_m \in V\). Prove that
    \[\{v_1, \ldots, v_m\}^\perp  = (\Span (v_1, \ldots, v_m))^\perp. \]
\end{problem}

\begin{proof}
Denote \(A = \{v_1, \ldots, v_m\}^\perp\) and \(B = \Span (v_1, \ldots, v_m)^\perp\)

\(\Rightarrow\) Let \( v \in A\), then \(\langle v,v_i \rangle =0 \) for all \(i\). So we have
\[\left\langle v,\sum_{i=1}^{m} a_i v_i \right\rangle = \sum_{i=1}^{m} \overline{a_i} \langle v,v_i \rangle = 0\]
which means that \(v \in B\).

\(\Leftarrow\) Conversely, let \(v \in B\), then naturally by definition \(v \in A\).
\end{proof}

\begin{problem}{4}
    Suppose \(e_1, \ldots, e_n\) is a list of vectors in \(V\) with \(\norm{e_k} = 1\) for each
    \(k = 1, \ldots, n\) and
    \[\norm{v}^2 = | \langle v,e_1 \rangle |^2 + \cdots + |\langle v,e_n \rangle|^2\]

    for all \(v \in V\). Prove that \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\).
\end{problem}

\begin{proof}
It now suffices to prove that \(\langle e_i,e_j \rangle = \delta_{ij}\). To see this, take \(v = e_i\),
then we have that
\[\norm{v}^2 = \norm{e_i}^2 = 1 = \sum_{j \neq i}^{n} |\langle e_i,e_j \rangle|^2 + 1 \]
This gives that \(\langle e_i,e_j \rangle = 0\) for all \(i \neq j\), completing the proof.
\end{proof}

\begin{problem}{5}
    Suppose that \(V\) is finite-dimensional and \(U\) is a subspace of \(V\). Show that
    \(P_{U^\perp} = I - P_U\), where \(I\) is the identity operator on \(V\).
\end{problem}

\begin{proof}
Take \(v \in V\), then we know \(v = u + w\) for \(u \in U, w \in U^\perp\). We have that
\[P_U v = u \ \ \ P_{U^\perp} v = w = v - u = (I - P_U)v\]
\end{proof}

\begin{problem}{6}
    Suppose \(V\) is finite-dimensional and \(T \in \Lc(V, W)\). Show that
    \[T = TP_{(\nul T)^\perp} = P_{\range T}T.\]
\end{problem}

\begin{proof}
Take arbitrary \(v \in V\), then \(v = u + w\) for \(u \in \nul T\) and \(w \in (\nul T)^\perp\). We have
\[Tv = T(u + w) = Tw = T P_{(\nul T)^\perp}v\]

Furthermore, since \(Tv \in \range T\), \(P_{\range T}\) acts as an identity operator for \(Tv\), thus
we have the second equality.
\end{proof}

\begin{problem}{7}
    Suppose that \(X\) and \(Y\) are finite-dimensional subspaces of \(V\). Prove that
    \(P_XP_Y = 0\) if and only if \(\langle x,y \rangle = 0\) for all \(x \in X\) and all \(y \in Y\).
\end{problem}

\begin{proof}
\(\Rightarrow\) take arbitrary \(y \in Y\), then \(P_X (y) = 0\), which means that \(y = 0 + (y - 0)\) where
\(0 \in X\) and thus all \(y \in Y\) are orthogonal to \(x \in X\), completing this direction.

\(\Leftarrow\) Take \(v \in V\), then \(v = y + y'\) for \(y \in Y, y' \in Y^\perp\) and we further have
\(y = 0 + y\) for \(0 \in X\) and \(y \in X^\perp\). We now have that
\[P_XP_Y(v) = P_X(y)  = 0\]
\end{proof}

\begin{problem}{9}
    Suppose \(V\) is finite-dimensional. Suppose \(P \in \Lc(V)\) is such that \(P^2 = P\) and
    every vector in \(\nul P\) is orthogonal to every vector in \(\range P\). Prove that there
    exists a subspace \(U\) of \(V\) such that \(P = P_{U}\).
\end{problem}

\begin{proof}
We can simply take \(U = \range P\). Note that \(V = \nul P \oplus \range P\) as
\[v = Pv + (v - Pv)\]
where \(P(v - Pv) = 0\) so \(v - Pv \in \nul P\).

Then take \(v = v_1 + v_2\) where \(v_1 \in \nul P, v_2 \in \range P\), then we have
\[Pv = P(v_1 + v_2) = Pv_2 = P_U v \]
\end{proof}

\begin{problem}{11}
    Suppose \(T \in \Lc(U)\) and \(U\) is a finite-dimensional subspace of \(V\). Prove that
    \[U \text{ is invariant under } T \Longleftrightarrow P_U T P_U = T P_U\]
\end{problem}

\begin{proof}
\(U\) is invariant under \(T\) \(\Longleftrightarrow\) \(Tu \in U\) for all \(u \in U\)
\(\Longleftrightarrow\) for \(v = u + u^\perp \in V\), \(TP_U (v) = Tu = P_U(Tu) = P_U T P_U (v) \)
\end{proof}

\begin{problem}{13}
    Suppose \(\F = \R\) and \(V\) is finite-dimensional. For each \(v \in V\), let \(\varphi_v\) denote
    the linear functional on \(V\) defined by
    \[\varphi_v(u) = \langle u,v \rangle\]
    for all \(u \in V\).
    \begin{enumerate}[label=(\alph*)]
        \item Show that \(v \mapsto \varphi_v\) is an injective linear map from \(V\) to \(V'\).
        \item Use (a) and a dimension-counting argument to show that \(v \mapsto \varphi_v\) is an
        isomorphism from \(V\) onto \(V'\).
    \end{enumerate}
\end{problem}

\begin{proof}
(a) denote this map \(v \mapsto \varphi_v\) to be \(T\). Then take \(v \in \nul T\), we have
\(T(v) = \varphi_v = 0\). By definition, since this holds for all \(u \in V\), \(v = 0\) and thus
\(T\) is injective. To show it's also linear, \(T (\lambda v_1 + v_2) (u)
= \varphi_{\lambda v_1 + v_2} (u) = \langle u, \lambda v_1 + v_2 \rangle = \lambda \langle u,v_1 \rangle
+ \langle u,v_2 \rangle = \lambda \varphi_{v_1} + \varphi_{v_2} = \lambda T(v_1) + T(v_2) \).

(b) We know that \(\dim V = \dim V'\) and combining this with (a) yields the solution.
\end{proof}

\begin{problem}{15}
    In \(\R^4\), let
    \[U = \Span((1,1,0,0),(1,1,1,2))\]
    Find \(u \in U\) such that \(\norm{u - (1,2,3,4)}\) is as small as possible.
\end{problem}

\begin{proof}
We first find the orthonormal basis of \(U\) and apply the formula, i.e. \(P_U(v) = \sum_{i=1}^{n}
\langle v,e_i \rangle e_i\). Using the Gram-Schmidt, we can find that
\[\left\{\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0, 0 \right), \left(0, 0, \frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}\right)\right\}\]
Then we can get that
\begin{align*}
    u &= \left\langle (1,2,3,4),\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0, 0\right) \right\rangle
\left(\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0, 0 \right) +  \\
&\quad \left\langle (1,2,3,4),\left(0, 0, \frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}\right) \right\rangle
\left(0, 0, \frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}\right) \\
&= \left(\frac{3}{2}, \frac{3}{2}, \frac{11}{5}, \frac{22}{5}\right)
\end{align*}
\end{proof}

% \begin{problem}{17}
%     Find \(p \in \Pc_3 (\R)\) such that \(p(0) = 0, p'(0) = 0\), and \(\int_{0}^{1}
%     |2 + 3x - p(x)|^2 dx\) is as small as possible.
% \end{problem}

% \begin{proof}
% The first two condition imply that \(p\) doesn't have constant/degree-1 terms. So we can consider the
% basis \(x^2, x^3\) and the inner product
% \[\langle p,q \rangle = \int_0^1 pq dx\]
% We follow the same procedure as the previous problem. Here we try to find \(P_U(2 + 3x)\) where \(U
% = \Span(x^2, x^3)\). First we obtain the orthornal basis:
% \[\frac{1}{\sqrt{5}}x^2, x^\]
% \end{proof}

\begin{problem}{19}
    Suppose \(V\) is finite-dimensional and \(P \in \Lc(V)\) is an orthogonal projection of \(V\) onto
    some subspace of \(V\). Prove that \(P^\dagger = P\).
\end{problem}

\begin{proof}
Suppose the subspace is \(U\). Take \(u \in U\), then we know that \(u \in \range P\), and thus
\begin{align*}
    P^\dagger u = (P|_{(\nul P)^\perp})^{-1} P_{\range P} u =  (P|_{(\nul P)^\perp})^{-1} u  = u = Pu
\end{align*}

Take \(u \in U^\perp\), then we have \(Pu = 0\) and also \(P^\dagger u = 0\) by definition. Thus these two operators
equal each other.
\end{proof}

\begin{problem}{20}
    Suppose \(V\) is finite-dimensional and \(T \in \Lc(V, W)\). Show that
    \[\nul T^\dagger = (\range T)^\perp \text{  and  } \range T^\dagger = (\nul T)^\perp.\]
\end{problem}

\begin{proof}
We know that \(T^\dagger = (T|_{(\nul T)^\perp})^{-1} P_{\range T}\) and the first part \(T|_{(\nul T)^\perp}\)
we've shown it's bijective with the restriction in book's lemma. So for \(v \in \nul T^\dagger\),
\(P_{\range T}v = 0\) and thus \(v \in (\range T)^\perp\). Conversely, it holds by definition.

For the other equality, take \(v \in \range T^\dagger\), then there exists \(u \in \range T\) s.t.
\(T|_{(\nul T)^\perp} v = u\), so \(v \in (\nul T)^\perp\). Conversely, take \(v \in (\nul T)^\perp\), then
there exists \(u \in \range T\) s.t. \(Tv = u\), and we have \(T^\dagger u =v\) so \(v \in \range T^\dagger\).
\end{proof}

\begin{problem}{22}
    Suppose \(V\) is finite-dimensional and \(T \in \Lc(V, W)\). Prove that
    \[TT^\dagger T = T \text{  and  } T^\dagger T T^\dagger = T^\dagger.\]
\end{problem}

\begin{proof}
For the first equality, take \(v \in \nul T\), then \(TT^\dagger T v = Tv = 0\). Take \(v \in (\nul T)^\perp\),
then \(TT^\dagger (Tv) = T(v)\) by definition.

For the second equality, take \(w \in (\range T)^\perp\), then \(T^\dagger T T^\dagger w = 0 = T^\dagger w\).
Take nonzero \(w \in \range T\), then there exists \(v \in (\nul T)^\perp\) such that \(Tv = w\), hence
\(T^\dagger T T^\dagger w = T^\dagger Tv = v = T^\dagger w\).
\end{proof}

\begin{problem}{23}
    Suppose \(V\) and \(W\) are finite-dimensional and \(T \in \Lc(V, W)\). Prove that
    \[(T^\dagger)^\dagger = T.\]
\end{problem}

\begin{proof}
Denote \(S = T^\dagger\), we have that
\[(T^\dagger)^\dagger = S^\dagger = (S|_{(\nul S)^\perp})^{-1} P_{\range S}
= (S|_{\range T})^{-1} P_{(\nul T)^\perp}\]
\end{proof}

where we use the conclusion from problem 20. Note that if \(v \in \nul T\), then naturally
\((T^\dagger)^\dagger v = Tv = 0\). If \(v \in (\nul T)^{\perp}\), then first note that

\[(S|_{\range T})^{-1} P_{(\nul T)^\perp} = (S|_{\range T})^{-1} v \]
Expanding the definition gives that
\[(S|_{\range T})^{-1}v = (((T|_{(\nul T)^\perp})^{-1} P_{\range T})|_{\range T})^{-1} v
= T|_{(\nul T)^\perp}v = Tv\]
Therefore, we complete the proof.

\end{document}
