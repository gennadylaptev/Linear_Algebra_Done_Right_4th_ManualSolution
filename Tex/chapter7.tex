\documentclass{extarticle}
\sloppy
\input{packages.tex}
\input{math_commands.tex}

\title{\vspace{-2em}Chapter 7: Operators on Inner Product Spaces}
\author{\emph{Linear Algebra Done Right (4th Edition)}, by Sheldon Axler}
\date{Last updated: \today}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7A %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{7A: Self-Adjoint and Normal Operators}
\addcontentsline{toc}{section}{7A: Self-Adjoint and Normal Operators}

\begin{definition}[adjoint, \(T^*\)]
    Suppose \( T \in \Lc(V, W)\). The \textbf{adjoint} of \(T\) is the function \(T^* \colon W \to V\) such
    that
    \[\langle Tv,w \rangle = \langle v,T^* w \rangle\]
    for every \(v \in V\) and every \(w \in W\).
\end{definition}

\begin{corollary}
    If \(T \in \Lc(V, W)\), then \(T^* \in \Lc(W, V)\). In other words, the adjoint of
    a linear map is a linear map.
\end{corollary}

\begin{corollary}[properties of the adjoint]
    Suppose \(T \in \Lc(V, W)\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \((S + T)^* = S^* + T^*\) for all \(S \in \Lc(V, W)\);
        \item \((\lambda T)^* = \overline{\lambda} T^*\) for all \(\lambda \in \F\);
        \item \((T^*)^* = T\);
        \item \((ST)^* = T^* S^*\) for all \(S \in \Lc(W, U)\) (here \(U\) is a finite-dimensional
        inner product space over \(\F\)).
        \item \(I^* = I\);
        \item if \(T\) is invertible, then \(T^*\) is invertible and \((T^*)^{-1} = (T^{-1})^*\).
    \end{enumerate}
\end{corollary}

\begin{thm}[null space and range of \(T^*\)]
Suppose \(T \in \Lc(V, W)\). Then
\begin{enumerate}[label=(\alph*)]
    \item \(\nul T^* = (\range T)^\perp\);
    \item \(\range T^* = (\nul T)^\perp\);
    \item \(\nul T = (\range T^*)^\perp\);
    \item \(\range T = (\nul T^*)^\perp\).
\end{enumerate}
\end{thm}

\begin{definition}[conjugate transpose, \(A^*\)]
    The \textbf{conjugate transpose} of an \(m\)-by-\(n\) matrix \(A\) is the \(n\)-by-\(m\) matrix
    \(A^*\) obtained by interchanging the rows and columns and then taking the complex conjugate
    of each entry. In other words, if \(j \in \{1, \ldots, n\}\) and \(k \in \{1, \ldots, m\}\),
    then
    \[(A^*)_{j, k} = \overline{A_{k, j}}\]
\end{definition}

\begin{remark}
    We denote \(A^\top\) (transpose) when we know the matrix is real. Note that wrt. nonorthonormal
    bases, the matrix of \(T^*\) does not necessarily equal the conjugate transpose of the matrix
    of \(T\).
\end{remark}

\begin{thm}[matrix of \(T^*\) equals conjugate transpose of matrix of \(T\)]
    Let \(T \in \Lc(V, W)\). Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\) and
    \(f_1, \ldots, f_m\) is an orthonormal basis of \(W\). Then \(\Mc(T^*, (f_1, \ldots, f_m),
    (e_1, \ldots, e_n))\) is the conjugate transpose of \(\Mc(T, (e_1, \ldots, e_n), (f_1, \ldots, f_m))\).
    In other words,
    \[\Mc(T^*) = (\Mc(T))^*\]
\end{thm}

\begin{remark}
    Orthogonal complements and adjoints are concepts that are easier to work with than
    annihilators and dual maps in the context of inner product spaces.
\end{remark}

\begin{definition}[self-adjoint]
    An operator \( T \in \Lc(V)\) is called \textbf{self-adjoint} if \(T = T^*\). In other words,
    an operator \(T \in \Lc(V)\) is self-adjoint if and only if
    \[\langle Tv,w \rangle = \langle v,Tw \rangle\]
    for all \(v, w \in V\).
\end{definition}

\begin{corollary}
    Every eigenvalue of a self-adjoint operator is real.
\end{corollary}

\begin{corollary}
    Suppose \(V\) is a \textbf{complex} inner product space and \(T \in \Lc(V)\). Then
    \[\langle Tv,v \rangle = 0 \text{ for every } v \in V \Longleftrightarrow T = 0.\]
\end{corollary}

\begin{corollary}
    Suppose \(V\) is a \textbf{complex} inner product space and \(T \in \Lc(V)\). Then
    \[T \text{ is self-adjoint} \Longleftrightarrow \langle Tv,v \rangle \in \R \text{ for every } v \in V\]
\end{corollary}

\begin{remark}
    The above two corollaries do not hold for real inner product spaces.
\end{remark}

\begin{thm}
    Suppose \(T\) is a self-adjoint operator on \(V\). Then
    \[\langle Tv,v \rangle = 0 \text{ for every } v \in V \Longleftrightarrow T = 0\]
\end{thm}

\begin{definition}[normal]
    An operator on an inner product space is called \textbf{normal} if it commutes with its adjoint.
    In other words, \(T \in \Lc(V)\) is normal if \(TT^* = T^* T\).
\end{definition}

\begin{remark}
    Every self-adjoint opeartor is normal, but not vice versa.
\end{remark}

\begin{thm}
    Suppose \(T \in \Lc(V)\). Then
    \[T \text{ is normal } \Longleftrightarrow \norm{Tv} = \norm{T^* v} \text{ for every }v \in V\]
\end{thm}

\begin{corollary}[range, null space, and eigenvectors of a normal operator]
    Suppose \(T \in \Lc(V)\) is normal. Then
    \begin{enumerate}[label=(\alph*)]
        \item \(\nul T = \nul T^*\);
        \item \(\range T = \range T^*\);
        \item \(V = \nul T \oplus \range T\);
        \item \(T - \lambda I\) is normal for every \(\lambda \in \F\);
        \item if \(v \in V\) and \(\lambda \in \F\), then \(Tv = \lambda v\) if and only if
        \(T^* v = \overline{\lambda}v\).
    \end{enumerate}
\end{corollary}

\begin{thm}
    Suppose \(T \in \Lc(V)\) is normal. Then eigenvectors of \(T\) corresponding to distinct
    eigenvalues are orthogonal.
\end{thm}

\begin{thm}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Then \(T\) is normal if and only if there exist
    commuting self-adjoint operators \(A\) and \(B\) such that \(T = A + i B\).
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7A PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{7A Problem Sets}

\begin{problem}{1}
    Suppose \(n\) is a positive integer. Define \(T \in \Lc(\F^n)\) by
    \[T(z_1, \ldots, z_n) = (0, z_1, \ldots, z_{n-1})\]
    Find a formula for \(T^*(z_1, \ldots, z_n)\).
\end{problem}

\begin{proof}
We have that
\begin{align*}
    \langle T(x_1, \ldots, x_n),(y_1, \ldots, y_n) \rangle
    &= \langle (0, x_1, \ldots, x_{n-1}),(y_1, \ldots, y_n) \rangle \\
    &= x_1 y_2 + \cdots + x_{n-1}y_n \\
    &= \langle (x_1, \ldots, x_n), (y_2, \ldots, y_{n}, 0) \rangle \\
    &= \langle (x_1, \ldots, x_n), T^*(y_1, \ldots, y_n) \rangle
\end{align*}
where \(T^*(z_1, \ldots, z_n) = (z_2, \ldots, z_n, 0)\).
\end{proof}


\begin{problem}{2}
    Suppose \(T \in \Lc(V, W)\). Prove that
    \[T = 0 \Longleftrightarrow T^* = 0 \Longleftrightarrow T^*T = 0 \Longleftrightarrow TT^* = 0\]
\end{problem}

\begin{proof}
\begin{align*}
    T = 0
    &\Longleftrightarrow \langle Tv,v \rangle = 0 \text{ for all } v \\
    &\Longleftrightarrow \langle v,T^*v \rangle = 0 \text{ for all } v \\
    &\Longleftrightarrow T^* = 0 \\
    &\Longleftrightarrow T^*T = T^*(0) = 0 \\
    &\Longleftrightarrow TT^* = 0
\end{align*}
\end{proof}

\begin{problem}{4}
    Suppose \(T \in \Lc(V)\) and \(U\) is a subspace of \(V\). Prove that
    \[U \text{ is invariant under } T \Longleftrightarrow U^\perp \text{ is invariant under }T^*\]
\end{problem}

\begin{proof}
Let \(u \in U\) and \(w \in U^\perp\), then we know that
\[\langle Tu,w \rangle = \langle u,T^*w \rangle\]
Thus
\[\langle Tu,w \rangle = 0 \Longleftrightarrow \langle u,T^*w \rangle = 0\]
which implies the desired result.
\end{proof}

\begin{problem}{5}
    Suppose \(T \in \Lc(V, W)\). Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\) and
    \(f_1, \ldots, f_m\) is an orthonormal basis of \(W\). Prove that
    \[\norm{T e_1}^2 + \cdots + \norm{T e_n}^2 = \norm{T^* f_1 }^2 + \cdots + \norm{T^* f_m}^2\]
\end{problem}

\begin{proof}
Note that first we have
\[\norm{T e_k}^2 = \langle T e_k, T e_k \rangle
= \left\langle T e_k, \sum_{j=1}^{m} \langle T e_k, f_j \rangle f_j \right\rangle
= \sum_{j=1}^{m} |\langle T e_k, f_j \rangle|^2\]

At the same time, we have
\[ \langle Te_k, f_j \rangle = \langle e_k, T^* f_j \rangle\]
Combining the two equations yields that
\begin{align*}
    \sum_{i=1}^{n} \norm{T e_i}^2
    = \sum_{i=1}^{n} \sum_{j=1}^{m} |\langle T e_i,f_j \rangle|^2
    = \sum_{j=1}^{m} \sum_{i=1}^{n} |\langle e_i,T^* f_j \rangle|^2
    = \sum_{j=1}^{m} \norm{T^* f_j}^2
\end{align*}
\end{proof}

\begin{problem}{9}
    Prove that the product of two self-adjoint operators on \(V\) is self-adjoint if and only if the
    two operatos commute.
\end{problem}

\begin{proof}
Let \(T\) and \(S\) be two self-adjoint operators on \(V\). First if \(ST = TS\), then \((ST)^*
= (TS)^*\). It suffices to show the forward direction. Let's suppose WLOG that \(ST\) is self-adjoint.
Then we know that \((ST)^* = T^* S^* = S^* T^* = (TS)^*\). This implies that for arbitrary
\(v, w \in V\), we have
\begin{align*}
    \langle v,T^* S^* w \rangle
    &= \langle v, S^* T^* w \rangle = \langle TSv,w \rangle \\
    &= \langle STv,w \rangle
\end{align*}
This implies that \(TS = ST\).
\end{proof}

\begin{problem}{10}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Prove that \(T\) is self-adjoint if and only if
    \[\langle Tv,v \rangle = \langle T^* v,v \rangle\]
    for all \(v \in V\).
\end{problem}

\begin{proof}
\begin{align*}
    T \text{ self-adjoint } \quad \Longleftrightarrow \quad
    T = T^* \quad \Longleftrightarrow \quad
    \langle Tv,v \rangle = \langle T^*v,v \rangle
\end{align*}
\end{proof}

\begin{problem}{12}
    An operator \(B \in \Lc(V)\) is called \textbf{skew} if
    \[B^* = - B\]
    Suppose that \(T \in \Lc(V)\). Prove that \(T\) is normal if and only if there exist commuting
    operators \(A\) and \(B\) such that \(A\) is self-adjoint, \(B\) is a skew operator, and
    \(T = A + B\).
\end{problem}

\begin{proof}
\(\Rightarrow\) Take \(A = \frac{T + T^*}{2}\) and \(B = \frac{T - T^*}{2}\), so it holds that
\(T = A + B\). We first verify that \(A\) is self-adjoint.
\begin{align*}
    4AA^* &= (T + T^*)(T + T^*)^* = (T + T^*)(T^* + T) = TT^* + TT + T^*T^* T^*T\\
    4A^*A &= (T + T^*)^* (T + T^*) = (T^* + T) (T + T^*) = T^*T + T^*T^* + TT + TT^*
\end{align*}
Therefore \(AA^* = A^*A\) and \(A\) is self-adjoint. For \(B\), we have that
\[B^* = \frac{T^* - T}{2} = - B\]

\(\Leftarrow\) We have that \(T^* = (A+B)^* = A^* - B = A - B\)
\begin{align*}
    TT^* &= (A+B)(A-B) = A^2 - AB + BA - B^2 = A^2 - B^2 \\
    T^*T &= (A - B)(A + B) = A^2 + AB - BA - B^2 = A^2 - B^2
\end{align*}
since \(A\) and \(B\) commute.
\end{proof}

\begin{problem}{15}
    Suppose \(T \in \Lc(V)\) is invertible. Prove that
    \begin{enumerate}[label=(\alph*)]
        \item \(T\) is self-adjoint \(\Longleftrightarrow\) \(T^{-1}\) is self-adjoint;
        \item \(T\) is normal \(\Longleftrightarrow\) \(T^{-1}\) is normal.
    \end{enumerate}
\end{problem}

\begin{proof}
(a) \(T\) is self-adjoint \(\Longleftrightarrow\) \(T = T^*\) \(\Longleftrightarrow\) \(T^{-1} = (T^*)^{-1} = (T^{-1})^*\)
\(\Longleftrightarrow\) \(T^{-1}\) is self-adjoint.

(b) \(T\) is normal \(\Longleftrightarrow\) \(TT^* = T^*T\) \(\Longleftrightarrow\)
\((T^*)^{-1}T^{-1} = T^{-1}(T^*)^{-1}\) \(\Longleftrightarrow\) \(T^{-1}\) is normal.
\end{proof}

\begin{problem}{19}
    Suppose \(T \in \Lc(V)\) and \(\norm{T^* v} \leq \norm{Tv}\) for every \(v \in V\). Prove that
    \(T\) is normal.
\end{problem}

\begin{proof}
% By result 7.20, it is equivalent to show that \(\norm{T^* v} = \norm{Tv}\) for every \(v \in V\). Assume
% for contradiction that there exists \(v \in V\) such that \(\norm{T^*v} < \norm{Tv}\). Then this
% implies that

Note that we have
\begin{align*}
    \norm{T^* v}^2 \leq \norm{T v}^2
    &\Longleftrightarrow \langle T^*v,T^* v \rangle \leq \langle Tv,Tv \rangle \\
    &\Longleftrightarrow \langle TT^*v,v \rangle \leq \langle T^* Tv,v \rangle \\
    &\Longleftrightarrow \langle (T^*T - TT^*)v,v \rangle \leq 0 \\
    &\Longleftrightarrow T^* T - TT^* \leq 0
\end{align*}

We want to borrow the fact from later chapters to make the proof substantially easier:
\[\tr(T^* T - TT^*) = 0 \]
which means that the sum of the eigenvalues are 0. However, by the fact derived above, let
\((\lambda, v)\) be arbitrary eigenpair of \(T^* T - TT^*\), we have that
\[\lambda \langle v,v \rangle = \langle (T^* T - TT^* )v,v \rangle \leq 0\]
meaning that \(\lambda \leq 0\) and therefore all \(\lambda = 0\). Notice that
\(T^*T - TT^*\) is self-adjoint as
\[(T^*T - TT^*)^* = (T^* T)^* - (TT^*)^* = T^* T - TT^*\]
Hence, we can claim that \(T^* T - TT^* = 0\), and by the result 7.20 \(T\) is normal.
\end{proof}

\begin{problem}{20}
    Suppose \(P \in \Lc(V)\) is such that \(P^2 = P\). Prove that the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(P\) is self-adjoint.
        \item \(P\) is normal.
        \item There is a subspace \(U\) of \(V\) such that \(P = P_U\).
    \end{enumerate}
\end{problem}

\begin{proof}
\((a) \Rightarrow (b)\) Trivial.

\((b) \Rightarrow (c)\) Take arbitrary \(v \in V\), then we have that
\[P(v - Pv) = Pv - P^2 v = 0\]
which means that \(v - Pv \in \nul P\). Since \(P\) is normal, we know that
\(V = \nul P \oplus \range P\) and thus every \(v\) can be written as
\[v = Pv + (v - Pv)\]
which means that \(P = P_{\range P}\).

\((c) \Rightarrow (a)\) Take any \(v_1, v_2 \in V\), then we know that \(v_1 = u_1 + w_1\) and
\(v_2 = u_2 + w_2\) for \(u_1, u_2 \in U\) and \(w_1, w_2 \in U^\perp\). It suffices to show that
\(\langle Pv_1,v_2 \rangle = \langle v_1, P v_2 \rangle\).
\begin{align*}
    \langle Pv_1,v_2 \rangle
    &= \langle u_1, u_2 + w_2 \rangle \\
    &= \langle u_1, u_2 \rangle + \langle w_1,u_2  \rangle \\
    &= \langle v_1,Pv_2 \rangle
\end{align*}
\end{proof}

\begin{problem}{23}
    Suppose \(T\) is a normal operator on \(V\). Suppose also that \(v, w \in V\) satisfy the
    equations
    \[\norm{v} = \norm{w} = 2, \ \ Tv = 3v, \ \ Tw = 4w\]
    Show that \(\norm{T(v+w)} = 10\).
\end{problem}

\begin{proof}
We note that \(v\) and \(w\) are distinct eigenvectors with different eigenvalues. As \(T\) is normal,
\(v\) and \(w\) are thus orthogonal. We have that
\begin{align*}
    \norm{T(v+w)}^2 = \norm{Tv}^2 + \norm{Tw}^2 = 9 \norm{v}^2 + 16 \norm{w}^2 = 100
\end{align*}
which shows the desired result.
\end{proof}

\begin{problem}{25}
    Suppose \(T \in \Lc(V)\). Prove that  \(T\) is diagnoalizable if and only if \(T^*\) is diagnoalizable.
\end{problem}

\begin{proof}
We know that \((\lambda, v)\) is an eigenpair of \(T\) if and only if \((\overline{\lambda},v)\) is
an eigenpair of \(T^*\). Since \(T\) and \(T^*\) shares the same eigenvectors, \(V\) has eigenbasis
consisting of \(T\)'s eigenvectors iff \(V\) has eigenbasis consisting of \(T^*\)'s eigenvectors,
completing the proof.
\end{proof}

\begin{problem}{27}
    Suppose \(T \in \Lc(V)\) is normal. Prove that
    \[\nul T^k = \nul T \text{ and } \range T^k = \range T\]
    for every positive integer \(k\).
\end{problem}

\begin{proof}
It is obvious that \(\nul T \subseteq \nul T^k\) and \(\range T^k \subseteq
\range T\). We aim at proving the other direction.

If \(T\) is self-adjoint, then the inclusion for other direction of null space is simpler to prove.
Suppose \(v \in \nul T^k\), then
\[\langle T^k v, T^{k-2} v \rangle = \langle T^{k-1} v, T^{k-1}v \rangle = 0\]
which shows that \(T^{k-1} v= 0\). Doing this recursively gives that \(T v = 0\).

Now let's consider the normal operator \(T\), then
\[(T^* T)^k v = (T^*)^k T^k v = 0\]
So \(v \in \nul (T^* T)^k\). Notice that \((T^* T)\) is a self-adjoint operator and applying the fact
proved above yields that \(v \in \nul T^* T \). Hence we have that
\[\langle T^* Tv,v \rangle = \langle Tv, Tv \rangle = 0\]
which shows that \(v \in \nul T\).

For proving the range-related inclusion, this can relate to the null space, where we have that
\begin{align*}
    \range (T^k) = (\nul (T^k)^*)^\perp = (\nul (T^*)^k)^\perp = (\nul T^*)^\perp = \range T
\end{align*}
\end{proof}

\begin{problem}{29}
    Prove or give a counterexample: If \(T \in \Lc(V)\)  and there is an orthonormal
    basis \(e_1, \ldots, e_n\) of $V$ such that \(\norm{T e_k} = \norm{T^* e_k}\) for each
    \(k = 1, \ldots, n\), then $T$ is normal.
\end{problem}

\begin{proof}
    We will give a counterexample to the statement. Let \( v \in V \). Then we have
    \begin{align*}
        \norm{Tv}^2 = \langle Tv, Tv \rangle =
        \left\langle
            T \left(\sum_{j=1}^{n} \langle v, e_j \rangle e_j\right),
            T \left(\sum_{k=1}^{n} \langle v, e_k \rangle e_k \right)
        \right\rangle
        \\ =
        \left\langle
            \sum_{j=1}^{n} \langle v, e_j \rangle  T e_j,
            \sum_{k=1}^{n} \langle v, e_k \rangle T e_k
        \right\rangle
        =
        \sum_{j=1}^{n}\sum_{k=1}^{n}
        \langle v, e_j \rangle  \overline {\langle v, e_k \rangle}
        \langle T e_j, T e_k \rangle.
    \end{align*}
    Similarly, \( \norm{T^* v}^2 = \sum_{j=1}^{n}\sum_{k=1}^{n}
        \langle v, e_j \rangle  \overline {\langle v, e_k \rangle}
        \langle T^* e_j, T^* e_k \rangle \).
        So, in order to give a counterexample
        we need to find some \( T \) such that
        it satisfies the hypothesis and
        \(\sum_{j \ne k} \langle T e_j, T e_k \rangle
        \ne \sum_{j \ne k} \langle T^* e_j, T^* e_k \rangle \).

    Now let \(V = \R^2\), \(e_1, e_2\) be a standart basis of \( \R^2 \) and
    \( a,  b,  c,  d \in \R \). Consider
    \begin{align*}
        T =
        \begin{pmatrix}
            a & b \\
            c & d
        \end{pmatrix}, \
        T^* =
        \begin{pmatrix}
            a & c \\
            b & d
        \end{pmatrix},
    \end{align*}
    and suppose that \( T \) satisfies the conditions above.
    Namely
    \begin{gather*}
        \norm{T e_1}^2 = \norm{T^* e_1}^2 \\
        \norm{T e_2}^2 = \norm{T^* e_2}^2 \\
        \langle T e_1, T e_2 \rangle =
        \frac{1}{2}\langle T e_1, T e_2 \rangle + \frac{1}{2}\langle T e_2, T e_1 \rangle
        \ne \\
        \frac{1}{2}\langle T^* e_1, T^* e_2 \rangle + \frac{1}{2}\langle T^* e_2, T^* e_1 \rangle =
        \langle T^* e_1, T^* e_2 \rangle.
    \end{gather*}
    The identities in the third line hold because \( \F=\R \).
    Plugging \( T \) we get
    \begin{gather*}
        a^2 + c^2 = a^2 + b^2 \quad \text{and} \quad  b^2 + d^2 = c^2 + d^2
        \implies b^2 = c^2 \\
        ab + cd \ne ac + bd.
    \end{gather*}
    To ensure \(T \ne T^*\) we take \( b = -c \).
    Then \( ab - bd = b(a-d) \ne -ab + bd = -b(a-d) \). So picking \( a \ne d \) and \( b \ne 0\)
    will suffice.
    For example, take \( a = 0, \ b = 1, \ c = -1, \ d = 1 \). Then we get the following
    matrices
    \begin{align*}
        T =
        \begin{pmatrix}
            0 & 1 \\
            -1 & 1
        \end{pmatrix}, \
        T^* =
        \begin{pmatrix}
            0 & -1 \\
            1 & 1
        \end{pmatrix}.
    \end{align*}
    You can easily verify that \(\norm{T e_k} = \norm{T^* e_k}\) for \( k=1, 2 \), but
    $ T T^* \ne T^* T$.
    Thus, \( T \) satisfies the hypothesis but is not normal.
\end{proof}

\begin{problem}{30}
    Suppose that \(T \in \Lc(\F^3)\) is normal and \(T(1, 1 ,1) = (2, 2, 2)\). Suppose
    \((z_1, z_2, z_3) \in \nul T\). Prove that \(z_1 + z_2 + z_3 = 0\).
\end{problem}

\begin{proof}
We can see that \((1,1,1)\) is an eigenvector of \(T\) with \(2\) to be the eigenvalue. Then
\(z_1, z_2, z_3\) is orthogonal to \((1,1,1)\), so we have that
\[z_1 + z_2 + z_3 = 0\]
by taking the inner product.
\end{proof}

\begin{problem}{32}
    Suppose \(T \colon V \to W\) is a linear map. Show that under the standard identification of
    \(V\) with \(V'\) and the corresponding identification of \(W\) with \(W'\), the adjoint map
    \(T^* \colon W \to V\) corresponds to the dual map \(T' \colon W' \to V'\). More precisely, show
    that
    \[T'(\varphi_w) = \varphi_{T^* w}\]
    for all \(w \in W\), where \(\varphi_w\) and \(\varphi_{T^* w}\) are defined previously in the
    book.
\end{problem}

\begin{proof}
We first know that \(T'(\varphi_w) = \varphi_w \circ T\), so
\[T'(\varphi_w)(u) = \langle Tu, w \rangle = \langle u, T^* w \rangle = \varphi_{T^* w}(u)\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7B %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{7B: Spectral Theorem}
\addcontentsline{toc}{section}{7B: Spectral Theorem}

\begin{lemma}[invertible quadratic expressions]
    Suppose \(T \in \Lc(V)\) is self-adjoint and \(b, c \in \R\) are such that \(b^2 < 4c\). Then
    \[T^2 + bT + cI\]
    is an invertible operator.
\end{lemma}

\begin{lemma}[minimal polynomial of self-adjoint operator]
    Suppose \(T \in \Lc(V)\) is self-adjoint. Then the minimal polynomial of \(T\) equals
    \((z - \lambda_1)\cdots(z - \lambda_m)\) for some \(\lambda_1, \ldots, \lambda_m \in \R\).
\end{lemma}

\begin{thm}[\textcolor{red}{REAL SPECTRAL THEOREM}]
Suppose \(\F = \R\) and \(T \in \Lc(V)\). Then the following are equivalent.
\begin{enumerate}[label=(\alph*)]
    \item \(T\) is self-adjoint.
    \item \(T\) has a diagonal matrix with respect to some orthonormal basis of \(V\).
    \item \(V\) has an orthonormal basis consisting of eigenvectors of \(T\).
\end{enumerate}
\end{thm}

\begin{thm}[\textcolor{red}{COMPLEX SPECTRAL THEOREM}]
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(T\) is normal.
        \item \(T\) has a diagonal matrix with respect to some orthonormal basis of \(V\).
        \item \(V\) has an orthonormal basis consisting of eigenvectors of \(T\).
    \end{enumerate}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7B PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{7B Problem Sets}


\begin{problem}{1}
    Prove that a normal operator on a complex inner product space is self-adjoint if and only if
    all its eigenvalues are real.
\end{problem}

\begin{proof}
\(\Rightarrow\) Assume the normal operator \(T\) is self-adjoint, then we know that \(T = T^*\) and
their eigenvalue and eigenvectors are the same. However, we know that \((\lambda, v)\) is an eigenpair
of \(T\) if and only if \((\overline{\lambda}, v)\) is an eigenpair of \(T^*\). This means that
\(\lambda = \overline{\lambda}\) and thus all eigenvalues are real.

\(\Leftarrow\) Conversely, since \(T\) is normal, \(T\) has a diagnoal entry wrt some orthonormal
basis of \(V\) where the entries are the eigenvalues. Since the eigenvalues are real, the conjugate transpose of
the matrix equals itself, therefore completing the proof.
\end{proof}

\begin{problem}{3}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\) is normal. Prove that the set of eigenvalues
    of \(T\) is contained in \(\{0, 1\}\) if and only if there is a subspace \(U\) of \(V\) such
    that \(T = P_U\).
\end{problem}

\begin{proof}
Since \(T\) is normal, there exists a diagonal matrix representation of \(T\) wrt. some orthonormal
basis \(e_1, \ldots, e_n\). Then we have that
\begin{align*}
    \text{eigenvalues of } T \text{ contained in } \{0, 1\}
    &\Longleftrightarrow Te_k = 0 \text{ or } T e_k = 1 \\
    &\Longleftrightarrow \exists U = \Span\{e_i\}_{i \in J} \text{ for some index set}\\
    &J \text{ s.t. } Tv \in U \text{ for all } v \in V
\end{align*}
\end{proof}

\begin{problem}{4}
    Prove that a normal operator on a complex inner product space is skew (meaning it equals the negative
    of its adjoint) if and only if all its eigenvalues are purely imaginary (meaning that they have
    real part equal to 0).
\end{problem}

\begin{proof}
Suppose \(T\) is normal, then we know that \(\lambda\) is an eigenvalue of \(T\) if and only if
\(\overline{\lambda}\) is an eigenvalue of \(T^*\).
\begin{align*}
    T \text{ is skew }
    &\Longleftrightarrow -\langle Tv,v \rangle = \langle T^*v,v \rangle = - \lambda \langle v,v \rangle \\
    &\Longleftrightarrow -\lambda = \overline{\lambda} \\
    &\Longleftrightarrow \lambda \text{ is purely imaginary}
\end{align*}
\end{proof}

\begin{problem}{6}
    Suppose \(V\) is a complex inner product space and \(T \in \Lc(V)\) is a normal operator such
    that \(T^9 = T^8\). Prove that \(T\) is self-adjoint and \(T^2 = T\).
\end{problem}

\begin{proof}
Let \((\lambda, v)\) be an eigenpair of \(T\). Then we know that
\[T^9v = \lambda^9 v = T^8v = \lambda^8 v\]
Therefore \(\lambda^9 = \lambda^8\) and thus \(\lambda = \{0, 1\}\), then by P1 we know \(T\)
is self-adjoint and by P3 we know that \(T\) is a projection operator and thus \(T^2 = T\).
\end{proof}

\begin{problem}{8}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Prove that \(T\) is normal if and only if
    every eigenvector of \(T\) is also an eigenvector of \(T^*\).
\end{problem}

\begin{proof}
\(\Rightarrow\) This follows by the theorem on the book.

\(\Leftarrow\) Since we are concerned with the field \(\C\), there exists a matrix of \(T\) that
is upper-triangular wrt. some orthonormal basis \(e_1, \ldots, e_n\) such that
\(T e_j = \sum_{i=1}^{j} a_{ij} e_i\). In particular, \(e_1\) is an eigenvector of \(T\) and it is
also an eigenvector of \(T^*\). So the first column of \(T^*\) only has the first term nonzero. We can
repeat the statement for other eigenvectors of \(T\) to that of \(T^*\). Note that since the field
is taken over \(\C\), \(T\) is guaranteed to have \(n\) eigenvalues and therefore the corresponding
eigenvectors. This means that \(T\) and \(T^*\) are simultaneously diagonalizable and thus they
commute, completing the proof.
\end{proof}

\begin{problem}{10}
    Suppose \(V\) is a complex inner product space. Prove that every normal operator on \(V\)
    has a square root. (Note that an operator \(S \in \Lc(V)\) is called a square root of \(T
    \in \Lc(V)\) if \(S^2 = T\)).
\end{problem}

\begin{proof}
Suppose \(T\) is a normal operator on \(V\). By complex spectral theorem, \(T = \diag(\lambda_1,
\ldots, \lambda_n)\) wrt. to some orthonormal bases \(e_1, \ldots, e_n\). We can naturally define
an operator \(S\) such that
\[Se_k = \sqrt{\lambda_k} e_k\]
Then we have that
\begin{align*}
    S^2(v) = S\left(\sum_{i=1}^{n} a_i S(e_i)\right) = S\left(\sum_{i=1}^{n} a_i \sqrt{\lambda_i} e_i \right)
    = \sum_{i=1}^{n} a_i \lambda_i e_i = \sum_{i=1}^{n} a_i T(e_i) = T(v)
\end{align*}
\end{proof}

\begin{problem}{11}
    Prove that every self-adjoint operator on \(V\) has a cube root.
\end{problem}

\begin{proof}
We can do the similar proof as above. The only difference is that the self-adjoint operators have
real eigenvales and therefore their cube root is also real and uniquely determined.
\end{proof}

\begin{problem}{12}
    Suppose \(V\) is a complex vector space and \(T \in \Lc(V)\) is normal. Prove that if \(S\)
    is an operator on \(V\) that commutes with \(T\), then \(S\) commutes with \(T^*\).
\end{problem}

\begin{proof}
Let \(v\) be an eigenvector of \(T\) with eigenvalue of \(\lambda\). We have that
\[T(Sv) = STv = S (\lambda v) = \lambda(Sv)\]
So \(Sv\) is an eigenvector of \(T\) with eigenvalue \(\lambda\). This means that it is also an eigenvector
of \(T^*\) with eigenvalue \(\overline{\lambda}\). Hence, we have
\[T^*(Sv) = \overline{\lambda}Sv = S(\overline{\lambda} v) = S T^*v\]
completing the proof.
\end{proof}

\begin{problem}{14}
    Suppose \(\F = \R\) and \(T \in \Lc(V)\). Prove that \(T\) is self-adjoint if and only if
    all pairs of eigenvectors corresponding to distinct eigenvalues of \(T\) are orthogonal and
    \(V = E(\lambda_1, T) \oplus \cdots \oplus E(\lambda_m, T)\), where \(\lambda_1, \ldots, \lambda_m\)
    denote the distinct eigenvalues of \(T\).
\end{problem}

\begin{proof}
\(\Rightarrow\) Given an self-adjoint operator \(T\), we know that there exists an orthonormal basis
of \(V\) consisting of eigenvectors of \(T\) and thus we know that \(V = E(\lambda_1, T) \oplus
\cdots \oplus E(\lambda_m, T)\). Let's say we have arbitrary eigenpairs \((\lambda_1, v_1)\) and
\((\lambda_2, v_2)\) with \(\lambda_1 \neq \lambda_2\). Then
\begin{align*}
    0 = \langle Tv_1,v_2 \rangle - \langle v_1,Tv_2 \rangle
    = (\lambda_1 - \lambda_2) \langle v_1,v_2 \rangle
\end{align*}
Therefore, we have \(\langle v_1,v_2 \rangle = 0\).

\(\Leftarrow\) Conversely, Such eigenvectors form a basis of \(V\) and thus \(T\) is self-adjoint.
\end{proof}

\begin{problem}{16}
    Suppose \(\F = \C\) and \(\Ec \subseteq \Lc(V)\). Prove that there is an orthonormal basis of
    \(V\) with respect to which every element of \(\Ec\) has a diagonal matrix if and only if
    \(S\) and \(T\) are commuting normal operators for all \(S, T \in \Ec\).
\end{problem}

\begin{proof}
\(\Rightarrow\) Since \(S, T \in \Ec\) are diagonal, they commute and are normal.

\(\Leftarrow\) This side follows as what we've done in ch5E P2.
\end{proof}

\begin{problem}{19}
    Suppose \(T \in \Lc(V)\) is self-adjoint and \(U\) is a subspace of \(V\) that is invariant
    under \(T\).
    \begin{enumerate}[label=(\alph*)]
        \item Prove that \(U^\perp\) is invariant under \(T\).
        \item Prove that \(T|_U \in \Lc(V)\) is self-adjoint.
        \item Prove that \(T |_{U^\perp} \in \Lc(U^\perp)\) is self-adjoint.
    \end{enumerate}
\end{problem}

\begin{proof}
(a) Let \(u \in U\) and \(w \in U^\perp\), then
\[\langle u,Tw \rangle = \langle Tu,w \rangle = 0\]
since \(Tu \in U\). Thus \(Tw \in U^\perp\).

(b) Take \(u_1, u_2 \in U\), then we know that \(u_1, u_2 \in V\). Since \(T\) is self-adjoint, then
\[\langle Tu_1, u_2 \rangle = \langle u_1,Tu_2 \rangle\]
which shows that \(T|_U\) is self-adjoint.

(c) This follows similarly.
\end{proof}

\begin{problem}{21}
    Suppose that \(T\) is a self-adjoint operator on a finite-dimensional inner product space and
    that 2 and 3 are the only eigenvalues of \(T\). Prove that
    \[T^2 - 5T + 6I = 0\]
\end{problem}

\begin{proof}
    Since \( T \) is self-adjoint, it is diagonalizable, therefore its minimal polynomial \( p \)
    has the form of \( (z - \lambda_1) \ldots (z - \lambda_n) \),
    where \( \lambda_1, ..., \lambda_n \) are distinct (by 5.62). Since roots of a minimal
    polynomial are exactly eigenvalues of \( T \) (by 5.27),
    then we have \( p(z) = (z - 2)(z - 3) \). Thus
    \begin{equation*}
        0 = p(T) = (T - 2I)(T - 3I) = T^2 - 5T + 6I.
    \end{equation*}
\end{proof}

\begin{problem}{23}
    Suppose \(T \in \Lc(V)\) is self-adjoint, \(\lambda \in \F\), and \(\epsilon > 0\). Suppose there
    exists \(v \in V\) such that \(\norm{v} = 1\) and
    \[\norm{Tv - \lambda v} < \epsilon\]
    Prove that \(T\) has an eigenvalue \(\lambda'\) such that \(|\lambda - \lambda'| < \epsilon\).
\end{problem}

\begin{proof}
Since \(T\) is self-adjoint, there exists an orthonormal eigenbasis \(e_1, \ldots, e_n\)
with the corresponding eigenvalues \(\lambda_1, \ldots, \lambda_n\) and every \( \lambda_i \) is real.
Then we know that any \(v \in V\) can be expressed as \(v = \sum_{i=1}^{n} c_i e_i\), where
\( c_i = \langle v, e_i \rangle \). We have
\begin{gather*}
    1 = \norm{v}^2 = \sum_{i=1}^{n}{\lvert c_i \rvert}^2 ~ \mathrm{(by \ 6.24)}, \\
    T v = \sum_{i=1}^{n} c_i \lambda_i e_i.
\end{gather*}
Further, we have
\begin{align*}
    \norm{T v - \lambda v}^2 =
    \left\langle \sum_{i=1}^{n} c_i (\lambda_i - \lambda) e_i,
    \sum_{j=1}^{n} c_j (\lambda_j - \lambda) e_j\right\rangle
    \\ =
    \sum_{i=1}^{n} \lvert c_i \rvert^2 (\lambda_i - \lambda)^2
    = \sum_{i=1}^{n} \lvert c_i \rvert^2 (\lambda - \lambda_i)^2 < \epsilon^2.
\end{align*}
Now suppose that \(|\lambda - \lambda_i| \ge \epsilon\) for all \( i = 1, \dots n \). Then
\begin{align*}
    \norm{T v - \lambda v}^2 = \sum_{i=1}^{n} \lvert c_i \rvert^2 (\lambda - \lambda_i)^2
    \ge \sum_{i=1}^{n} \lvert c_i \rvert^2 \epsilon^2 =
    \epsilon^2 \sum_{i=1}^{n} \lvert c_i \rvert^2
    = \epsilon^2,
\end{align*}
which leads to contradiction. Thus, there exists some eigenvalue \( \lambda' \) such that
\(|\lambda - \lambda'| < \epsilon\).
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7C %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{7C: Positive Operators}
\addcontentsline{toc}{section}{7C: Positive Operators}

\begin{definition}[positive operator]
    An operator \(T \in \Lc(V)\) is called \textbf{positive} if \(T\) is self-adjoint and
    \[\langle Tv,v \rangle \geq 0\]
    for all \(v \in V\).
\end{definition}

\begin{remark}
    Positive operators are also known as positive definite operators.
\end{remark}

\begin{definition}[square root]
    An operator \(R\) is called a \textbf{squared root} of an operator \(T\) if \(R^2 = T\).
\end{definition}

\begin{thm}[characterizations of positive operators]
    Let \(T \in \Lc(V)\). Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(T\) is a positive operator.
        \item \(T\) is self-adjoint and all eigenvalues of \(T\) are nonnegative.
        \item With respect to some orthonormal basis of \(V\), the matrix of \(T\) is a diagonal
        matrix with only nonnegative numbers on the diagonal.
        \item \(T\) has a positive square root.
        \item \(T\) has a self-adjoint square root.
        \item \(T = R^* R\) for some \(R \in \Lc(V)\).
    \end{enumerate}
\end{thm}

\begin{thm}
    Every positive operator on \(V\) has a unique positive square root.
\end{thm}

\begin{remark}
    For \(T\) a positive operator, \(\sqrt{T}\) denotes the unique positive square root of \(T\).
\end{remark}

\begin{corollary}
    Suppose \(T\) is a positive operator on \(V\) and \(v \in V\) is such that \(\langle Tv,v \rangle = 0\).
    Then \(Tv = 0\).
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7C PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{7C Problem Sets}

\begin{problem}{1}
    Suppose \(T \in \Lc(V)\). Prove that if both \(T\) and \(-T\) are positive operators, then
    \(T = 0\).
\end{problem}

\begin{proof}
This means that
\[\langle Tv,v \rangle \geq 0 \ \ \ \langle Tv,v \rangle \leq 0\]
for all \(v \in V\), so \(T = 0\).
\end{proof}

\begin{problem}{3}
    Suppose \(n\) is a positive integer and \(T \in \Lc(\F^n)\) is the operator whose matrix (wrt.
    the standard basis) consists of all \(1\)'s. Show that \(T\) is a positive operator.
\end{problem}

\begin{proof}
Suppose \(v = (v_1, \ldots, v_n) \in V\), then we have that
\[\langle Tv,v \rangle
= \left\langle \left(\sum_{i=1}^{n} v_i, \ldots, \sum_{i=1}^{n} v_i \right), (v_1, \ldots, v_n) \right\rangle
= \left(\sum_{i=1}^{n} v_i \right)^2 \geq 0\]
\end{proof}

\begin{problem}{6}
    Prove that the sum of two positive operators on \(V\) is a positive operator.
\end{problem}

\begin{proof}
Let \(S, T\) be two positive operator and take \(v \in V\). Then
\[\langle (S+T)v,v \rangle = \langle Sv,v \rangle + \langle Tv,v \rangle \geq 0\]
\end{proof}

\begin{problem}{7}
    Suppose \(S \in \Lc(V)\) is an invertible positive operator and \(T \in \Lc(V)\) is a positive
    operator. Prove that \(S + T\) is invertible.
\end{problem}

\begin{proof}
Take any nonzero \(v \in V\), then
\[\langle (S+T)v,v \rangle = \langle Sv,v \rangle + \langle Tv,v \rangle > 0\]
So \(\nul (S+T) = \{0\}\) and thus \(S+T\) is injective and therefore invertible.
\end{proof}

\begin{problem}{9}
    Suppose \(T \in \Lc(V)\) is a positive operator and \(S \in \Lc(W, V)\). Prove that
    \(S^* T S\) is a positive operator on \(W\).
\end{problem}

\begin{proof}
We have that
\begin{align*}
    \langle S^*TS w,w \rangle
    = \langle T(Sw), (Sw) \rangle \geq 0
\end{align*}
as \(T\) is positive.
\end{proof}

\begin{problem}{10}
    Suppose \(T\) is a positive operator on \(V\). Suppose \(v, w \in V\) are such that
    \[Tv = w \ \ \ Tw = v\]
    Prove that \(v = w\).
\end{problem}

\begin{proof}
We have that
\[T^2 v = T(Tv) = Tw = v\]
Thus \((T^2 - I)v = 0\). This means that either \(v = 0\) or \(T = \pm I\). In the first case,
\(w = Tv = 0 = v\) and \(Tw = 0 = v\). In the second case, if \(T = I\), then \(v = w\). Note that
\(T\) cannot be \(-I\) as \(T\) is a positive operator (all eigenvalues nonnegative).
\end{proof}

\begin{problem}{12}
    Suppose \(T \in \Lc(V)\) is a positive operator. Prove that \(T^k\) is a positive operator
    for every positive integer \(k\).
\end{problem}

\begin{proof}
If \(k\) is even, then
\[\langle T^k v,v \rangle = \langle T^{\frac{k}{2}}v,T^{\frac{k}{2}}v \rangle \geq 0\]
If \(k\) is odd, then
\[\langle T^k v,v \rangle = \langle T(T^{\frac{k-1}{2}}v), (T^{\frac{k-1}{2}}v) \rangle \geq 0\]
as \(T\) is positive.
\end{proof}

\begin{problem}{13}
    Suppose \(T \in \Lc(V)\) is self-adjoint and \(\alpha \in \R\).
    \begin{enumerate}[label=(\alph*)]
        \item Prove that \(T - \alpha I\) is a positive operator if and only if \(\alpha\) is less
        than or equal to every eigenvalue of \(T\).
        \item Prove that \(\alpha I - T\) is a positive operator if and only if \(\alpha\) is
        greater than or equal to every eigenvalue of \(T\).
    \end{enumerate}
\end{problem}

\begin{proof}
Let \(v_1, \ldots, v_n\) be the eigenbasis of \(V\) with corresponding sorted eigenvalues \(\lambda_1,
\ldots, \lambda_n\) (\(\lambda_1\) being smallest). Note that \(\lambda\) is a eigenvalue of \(T\) iff \(T- \alpha I\) is
an eigenvalue of \(T - \alpha I\) iff \(\alpha - \lambda\) is an eigenvalue of \(\alpha I - T\). You
may verify this.

(a) \(T - \alpha I\) positive \(\Longleftrightarrow\) \(\lambda_1 - \alpha \geq 0\)
\(\Longleftrightarrow\) \(\alpha \leq \lambda_i\) for all \(i\).
(b) \(\alpha I  -T\) positive \(\Longleftrightarrow\) \(\alpha - \lambda_n \geq 0\)
\(\Longleftrightarrow\) \(\alpha \geq \lambda_i\) for all \(i\).
\end{proof}

\begin{problem}{14}
    Suppose \(T\) is a positive operator on \(V\) and \(v_1, \ldots, v_m \in V\). Prove that
    \[\sum_{j=1}^{m} \sum_{k=1}^{m} \langle Tv_k,v_j \rangle \geq 0\]
\end{problem}

\begin{proof}
\[\sum_{j=1}^{m} \sum_{k=1}^{m} \langle Tv_k,v_j \rangle
= \left\langle T\left(\sum_{i=1}^{m} v_i \right), \left(\sum_{i=1}^{m} v_i \right) \right\rangle \geq 0\]
\end{proof}

\begin{problem}{15}
    Suppose \(T \in \Lc(V)\) is self-adjoint. Prove that there exist positive operators \(A, B
    \in \Lc(V)\) such that
    \[T = A - B \text{ and } \sqrt{T^* T} = A + B \text{ and } AB = BA = 0\]
\end{problem}

\begin{proof}
Define \(|T| = \sqrt{T^2}\) and that
\[A = \frac{|T| + T}{2} \ \ \ B = \frac{|T| - T}{2}\]
So \(A - B = T\) and \(A+B = |T| = \sqrt{T^* T}\). Finally, we have
\[AB = \frac{|T| + T}{2} \frac{|T| - T}{2} = \frac{|T|^2 - |T|T + T|T| - T^2}{4} = 0\]
As \(|T|^2 = T^2\) and \(|T|T = T|T|\). Similarly, \(BA = 0\).
\end{proof}

\begin{problem}{16}
    Suppose \(T\) is a positive operator on \(V\). Prove that
    \[\nul \sqrt{T} = \nul T \text{  and  } \range \sqrt{T} = \range T\]
\end{problem}

\begin{proof}
We've shown that the unique square root \(\sqrt{T}\) is obtained from simply taking the square root
of the eigenvalues of \(T\). This means that \(\sqrt{T}\) and \(T\) share the same eigenbasis, which
determines the respective null and range space and completes the proof.
\end{proof}

\begin{problem}{18}
    Suppose \(S\) and \(T\) are positive operators on \(V\). Prove that \(ST\) is a positive
    operator if and only if \(S\) and \(T\) commute.
\end{problem}

\begin{proof}
\(\Rightarrow\) Given \(ST\) is positive, then
\[ST = (ST)^* = T^* S^* = TS \]

\(\Leftarrow\) Given \(ST = TS\), then \((ST)^* = T^* S^* = TS = ST\). In addition,
\[\langle ST v,v \rangle = \langle S\sqrt{T}v,\sqrt{T}v \rangle \geq 0\]
\end{proof}

\begin{problem}{22}
    Suppose \(T \in \Lc(V)\) is a positive operator and \(u \in V\) is such that
    \(\norm{u} =1\) and \(\norm{Tu} \geq \norm{Tv}\) for all \(v \in V\) with \(\norm{v} = 1\).
    Show that \(u\) is an eigenvector of \(T\) corresponding to the largest eigenvalue of \(T\).
\end{problem}

\begin{proof}
Let \(e_1, \ldots, e_n\) be the orthogonal eigenbasis of \(V\) and the corresponding eigenvalues
to be \(\lambda_1, \ldots, \lambda_n\) with sorted from smallest to largest. Then we have that
\[\norm{Tu} = \norm{\sum_{i=1}^{n} a_i \lambda_i e_i} = \sqrt{\sum_{i=1}^{n} |a_i|^2 \lambda_i^2}\]
We can take \(v = e_n\), then we have that
\[\norm{Tu}^2 = \sum_{i=1}^{n} |a_i|^2 \lambda_i^2 \geq \sum_{i=1}^{n} |a_i|^2 \lambda_n^2
= \lambda_n^2 = \norm{Tv}^2\]
which shows the desired conclusion.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7D %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{7D: Isometries, Unitary Operators, and Matrix Factorization}
\addcontentsline{toc}{section}{7D: Isometries, Unitary Operators, and Matrix Factorization}

\begin{definition}[isometry]
    A linear map \(S \in \Lc(V, W)\) is called an \textbf{isometry} if
    \[\norm{Sv} = \norm{v}\]
    for every \(v \in V\). In other words, a linear map is an isometry if it preserves norms.
\end{definition}

\begin{remark}
    Every isometry is injective.
\end{remark}

\begin{thm}[characterizations of isometries]
    Suppose \(S \in \Lc(V, W)\). Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\)
    and \(f_1, \ldots, f_m\) is an orthonormal basis of \(W\). Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(S\) is an isometry.
        \item \(S^* S = I\).
        \item \(\langle Su,Sv \rangle = \langle u,v \rangle\) for all \(u, v \in V\).
        \item \(Se_1, \ldots, Se_n\) is an orthonoral list in \(W\).
        \item The columns of \(\Mc(S, (e_1, \ldots, e_n), (f_1, \ldots, f_m))\) form an orthonormal
        list in \(\F^m\) with respect to the Euclidean inner product.
    \end{enumerate}
\end{thm}

\begin{definition}[Unitary Operators]
    An operator \(S \in \Lc(V)\) is called \textbf{unitary} if \(S\) is an invertible isometry.
\end{definition}


\begin{remark}
    Note that every isometry is injective and therefore invertible on a finite-dimensional vector
    space. The author makes the distinction simply to avoid confusion in more abstract stages. One
    might think of unitary operators being equivalent to isometries in finite-dimensional space.
\end{remark}

\begin{thm}[characterizations of unitary operators]
    Suppose \(S \in \Lc(V)\). Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\).
    Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(S\) is a unitary operator.
        \item \(S^* S = SS^* =  I\).
        \item \(S\) is invertible and \(S^{-1} =S^*\).
        \item \(Se_1, \ldots, Se_n\) is an orthonoral list in \(W\).
        \item The rows of \(\Mc(S, (e_1, \ldots, e_n))\) form an orthonormal
        basis of \(\F^m\) with respect to the Euclidean inner product.
        \item \(S^*\) is a unitary operator.
    \end{enumerate}
\end{thm}

\begin{corollary}
    Suppoe \(\lambda\) is an eigenvalue of a unitary operator. Then
    \(|\lambda| = 1\).
\end{corollary}

\begin{corollary}
    Suppose \(\F = \C\) and \(S \in \Lc(V)\). Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(S\) is a unitary operator.
        \item There is an orthonormal basis of \(V\) consisting of eigenvectors of \(S\) whose
        corresponding eigenvalues all have absolute value 1.
    \end{enumerate}
\end{corollary}

\begin{definition}[unitary matrix]
    An \(n\)-by-\(n\) matrix is called \textbf{unitary} if its columns form an orthonormal list in \(\F^n\).
\end{definition}

\begin{thm}[characterizations of unitary matrices]
    Suppose \(Q\) is an \(n\)-by-\(n\) matrix. Then the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(Q\) is a unitary matrix.
        \item The rows of \(Q\) form an orthonormal list in \(\F^n\).
        \item \(\norm{Qv} = \norm{v}\) for every \(v \in \F^n\).
        \item \(Q^* Q = Q Q^* = I_n\).
    \end{enumerate}
\end{thm}

\begin{thm}[QR factorization]
    Suppose \(A\) is a square matrix with linearly independent columns. Then there exists
    unique matrices \(Q\) and \(R\) such that \(Q\) is unitary, \(R\) is upper triangular
    with only positive numbers on its diagonal, and
    \[A = QR.\]
\end{thm}

\begin{lemma}[positive invertible operator]
    A self-adjoint operator \(T \in \Lc(V)\) is a positive invertible operator if and only
    if \(\langle Tv,v \rangle > 0\) for every nonzero \(v \in V\).
\end{lemma}

\begin{definition}[positive definite]
    A matrix \(B \in \F^{n, n}\) is called \textbf{positive definite} if \(B^* = B\) and
    \[\langle Bx,x \rangle > 0\]
    for every nonzero \(x \in \F^n\).
\end{definition}

\begin{thm}[Cholesky factorization]
    Suppose \(B\) is a positive definite matrix. Then there exists a unique upper-triangular
    matrix \(R\) with only positive numbers on its diagonal such that
    \[B = R^* R\]
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7D PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{7D Problem Sets}

\begin{problem}{1}
    Suppose \(\dim V \geq 2\) and \(S \in \Lc(V, W)\). Prove that \(S\) is an isometry if
    and only if \(S e_1, S e_2\) is an orthonormal list in \(W\) for every orthonormal
    list \(e_1, e_2\) of length two in \(V\).
\end{problem}

\begin{proof}
    $(\implies)$ Let \( e_1, e_2 \) be an orthonormal list in \( V \).
    Since \( S \) is an isometry, then \( S^* S = I \). Hence
    \begin{gather*}
        \langle S e_j, S e_k \rangle  = \langle e_j, S^* S e_k \rangle =
        \langle e_j, e_k \rangle = \delta_{j k},
    \end{gather*}
    where \( j = 1, 2, k = 1, 2 \). Thus, \( S e_1, S e_2 \) is an orthonormal list in \( W \).

    $(\impliedby)$ Let \( e_1, \dots, e_n \) be an orthonormal basis of \( V \). Then
    any list \( (e_j, e_k) \), where \( j \ne k \) and \( j=1, \dots, n, \ k = 1, \dots, n\) is
    orthonormal. By hypothesis, \( (S e_j, S e_k) \) is an orthonormal in \( W \). Hence,
    \( \langle S e_j, S e_k \rangle = \delta_{j k}\) for  \( j=1, \dots, n, \ k = 1, \dots, n\).
    Therefore, \( S e_1, \dots, S e_n \) is an orthonormal list in \( W \). Thus, \( S \) is an
    isometry.
\end{proof}

\begin{problem}{2}
    Suppose \(T \in \Lc(V, W)\). Prove that \(T\) is a scalar multiple of an isometry if and only
    if \(T\) preserves orthogonality.
\end{problem}

\begin{proof}
\(\Rightarrow\) if \(T = aS\) for \(S\) to be an isometry, then suppose \(\langle u,v \rangle = 0\),
\[\langle Tu,Tv\rangle = |a|^2 \langle Su,Sv \rangle = |a|^2 \langle u,v \rangle = 0\]

\(\Leftarrow\) Suppose for all \(\langle u,v \rangle = 0\), \(\langle Tu,Tv \rangle = 0\). Take
an orthonormal basis \(\{e_1, \ldots, e_n\}\) of \(V\). Then we have that
\[\langle e_i + e_j,e_i - e_j \rangle = \norm{e_i}^2 - \langle e_i,e_j \rangle +
\langle e_j,e_i \rangle - \norm{e_j}^2 = 1 - 1 = 0\]
This means that
\[\langle T(e_i + e_j), T(e_i - e_j) \rangle
= \norm{T e_i}^2 - \norm{T e_j}^2 = 0\]
which means that \(\norm{T e_k} = a\) for some scalar \(a\). Take any \(v \in V\), then
\[\norm{Tv}^2 = \sum_{i=1}^{n} |\langle v,e_i \rangle|^2 \norm{T e_i}^2 + \sum_{i \neq j}
|\langle v,e_i \rangle \langle v,e_j \rangle| \langle Te_i,Te_j \rangle
= \sum_{i=1}^{n} |\langle v,e_i \rangle|^2 |a|^2 = \norm{av}^2\]
This means that
\[\norm{Tv} = a \norm{v}\]
for some \(a\) and thus completes the proof (\(\frac{1}{a} T\) is an isometry).
\end{proof}

\begin{problem}{3}
    \begin{enumerate}[label=(\alph*)]
        \item Show that the product of two unitary operators on \(V\) is a unitary operator.
        \item Show that the inverse of a unitary operator on \(V\) is a unitary operator.
    \end{enumerate}
\end{problem}

\begin{proof}
Let \(T, S\) be two unitary operators.

(a) we have
\[(TS)^*(TS) = S^* T^* TS = S^*IS = S^*S = I\]

(b) since \(S^{-1} = S^*\) and \(S^*\) is a unitary operator, \(S^{-1}\) is also unitary.
\end{proof}

\begin{problem}{5}
    Suppose \(S \in \Lc(V)\). Prove that the following are equivalent.
    \begin{enumerate}[label=(\alph*)]
        \item \(S\) is a self-adjoint unitary operator.
        \item \(S = 2P - I\) for some orthogonal projection \(P\) on \(V\).
        \item There exists a subspace \(U\) of \(V\) such that \(Su = u\) for every
        \(u \in U\) and \(Sw = -w\) for every \(w \in U^\perp\).
    \end{enumerate}
\end{problem}

\begin{proof}
(a) \(\Rightarrow\) (b), (c) Since \(S\) is unitary, its eigenvalues are either -1 or 1. Since
\(S\) is self-adjoint, we can decompose \(V\) into its eigenspace \(E(-1)\) and
\(E(1)\) such that \(V = E(-1) \oplus E(1)\). This means that for all \(w \in E(-1),
Sw = - w\) and \(u \in E(1), S u = u\) where \(E(1) = U\). So part (c) is completed. For
each \(v \in V\), \(v = v_1 + v_{-1}\) where \(v_1 \in E(1), v_{-1} \in E(-1)\). Then define
\(P(v) = v_1\) and we have that \(Sv = Sv_1 + S v_{-1} = v_1 - v_{-1} = P(v) - (I - P)(v)
= (2P - I) v\). Part(b) is also completed.

(b), (c) \(\Rightarrow\) (a) Conversely, first assume (b) holds, then this means that
\[S^* = (2P - I)^* = 2P - I = S\]
so \(S\) is self-adjoint. To see that it is unitary,
\[SS^* = S^2 = (2P - I)(2P - I) = 4P^2 - 2P - 2P + I = I\]
Next assume (c) holds, then Let \(P\) define to be the projection operator into \(U\) and then
\((I - P)\) is the projection into \(U^\perp\), then we have that \(S = P - (I - P) = 2P - I\), so
applying the previous proof finishes the problem.
\end{proof}

\begin{problem}{6}
    Suppose \(T_1, T_2\) are both normal operators on \(\F^3\) with 2,5,7 as eigenvalues. Prove
    that there exists a unitary operator \(S \in \Lc(\F^3)\) such that \(T_1 = S^* T_2 S\).
\end{problem}

\begin{proof}
By complex spectral theorem, \(T_1\) has \(\{a_1, a_2, a_3\}\) as a set of orthonormal eigenvectors that
form a basis with eigenvalues 2,5,7. Similarly, \(T_2\) has \(\{b_1, b_2, b_3\}\). What we want to
do here is simply define a change-of-basis operator that is also unitary. Define \(S \in \Lc(\F^3)\) s.t.
\[S a_i = b_i\]
Clearly, \(S\) is unitary. Now, it suffices to prove that the desired equation holds.
We also have that \(S^{-1} = S^* \) so \(S^* b_i = a_i\). Take any
\(v = \alpha_1 a_1 + \alpha_2 a_2 + \alpha_3 a_3\), then
\begin{align*}
    S^*T_2S (v)
    &= S^*T_2 S \left(\sum_{i=1}^{3} \alpha_i a_i \right) \\
    &= S^*T_2 \left( \sum_{i=1}^{3} \alpha_i S(a_i) \right) \\
    &= S^* \left(\sum_{i=1}^{3} \alpha_i T_2(b_i) \right) \\
    &= S^* (\alpha_1 2b_1 + \alpha_2 5b_2 + \alpha_3 7b_3) \\
    &= 2 \alpha_1 a_1 + 5 \alpha_2 a_2 + 7 \alpha_3 a_3
\end{align*}

Note that we also have that

\begin{align*}
    T_1 (v)
    &= T_1 \left(\sum_{i=1}^{3} \alpha_i a_i \right) \\
    &= 2\alpha_1 a_1 + 5 \alpha_2 a_2 + 7 \alpha_3 a_3
\end{align*}
\end{proof}

\begin{problem}{9}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Suppose every eigenvalue of \(T\) has absolute value
    1 and \(\norm{Tv} \leq \norm{v}\) for every \(v \in V\). Prove that \(T\) is a unitary operator.
\end{problem}

\begin{proof}
Since \(T\) has no eigenvalue 0, it is invertible. We also have that
\begin{align*}
    \norm{Tv} \leq \norm{v}
    &\Longleftrightarrow \langle Tv,Tv \rangle \leq \langle v,v \rangle \\
    &\Longleftrightarrow \langle T^*T v,v \rangle \leq \langle v,v \rangle \\
    &\Longleftrightarrow \langle (T^* T - I)v,v \rangle \leq 0
\end{align*}
which gives that \(T^*T \leq I\). Since the eigenvalue of \(T^*T\) are all one and it is
self-adjoint, there exists eigenbasis all with eigenvalue one. This implies that \(T^*T = I\)
and thus \(T\) is unitary.
\end{proof}

\begin{problem}{10}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\) is a self-adjoint operator such that
    \(\norm{Tv} \leq \norm{v}\) for all \(v \in V\).
    \begin{enumerate}[label=(\alph*)]
        \item Show that \(I - T^2\) is a positive operator.
        \item Show that \(T + i \sqrt{I - T^2}\) is a unitary operator.
    \end{enumerate}
\end{problem}

\begin{proof}
(a)  take any \(v \in V\), then
\begin{align*}
    \langle (I - T^2)v,v \rangle
    &= \langle v,v \rangle - \langle Tv,T^*v \rangle \\
    &= \langle v,v \rangle - \langle Tv,Tv \rangle \\
    &= \langle (I - T)v, (I - T)v \rangle \geq 0
\end{align*}

(b) Let \(A = T, B = \sqrt{I - T^2}\). Then we have that
\[(A+Bi)(A+Bi)^* = A^2 -iAB + iBA + B^2\]
We know that \(A\) is self-adjoint so \(A^2\) is as well. By (a), \(B^2\) is also
self-adjoint. Note that \(AB = BA\) algebraically. Thus, we have that
\[(A+Bi)(A+Bi)^* = A^2 + B^2 = I\]
which shows that \(A+Bi\) is unitary.
\end{proof}

\begin{problem}{11}
    Suppose \(S \in \Lc(V)\). Prove that \(S\) is a unitary operator if and only if
    \[\{Sv \colon v \in V \text{ and } \norm{v} \leq 1\} = \{v \in V \colon \norm{v} \leq 1\}\]
\end{problem}

\begin{proof}
\(\Rightarrow\) take any \(v \in r.h.s.\), then since \(S\) is invertible and thus surjective, there exists
\(u \in V\) such that \(Sv = u\). As \(\norm{v} \leq 1\) and \(\norm{Su} = \norm{u} = \norm{v} \leq 1\),
so \(v \in l.h.s.\). Conversely, take any \(Sv \in l.h.s.\), then \(\norm{Sv} = \norm{v} \leq 1\), so
\(Sv \in r.h.s.\). Hence we complete this direction.

\(\Leftarrow\) Take any \(v \in V\) and consider \(u \coloneqq \frac{v}{\norm{v}}\). Then we know that
there exists \(w\) with \(\norm{w} \leq 1\) such that \(Sw = u \).
\end{proof}

\begin{problem}{13}
    Explain why the columns of a square matrix of complex numbers form an orthonormal list in \(\C^n\)
    if and only if the rows of the matrix form an orthonormal list in \(\C^n\).
\end{problem}

\begin{proof}
A square matirx \(Q\) is unitary iff \(Q^*\) is unitary, completing the proof.
\end{proof}

\begin{problem}{18}
    A square matrix \(A\) is called \emph{symmetric} if it equals its transpose. Prove that if
    \(A\) is a symmetric matrix with real entries, then there exists a unitary matrix \(Q\) with
    real entries such that \(Q^* A Q\) is a diagonal matrix.
\end{problem}

\begin{proof}
So we know that \(A\) is self-adjoint, so it is orthogonally diagonalizable. There exists eigenbasis
\(v_1, \ldots, v_n\) that are orthogonal. We can thus define \(Q\) to be consisting of such eigenvectors
as
\[Av_i = \lambda_i v_i\]
Writing this in matrix form gives that
\[AQ = QD\]
for unitary matrix \(Q\) and diagonal matrix \(D\) with entries to be the corresponding eigenvalues.
Hence, we get that
\[Q^*AQ\]
is diagonal.
\end{proof}

\begin{problem}{19}
    Suppose \( n \) is a positive integer. For this exercise, we adopt the notation that a typical
    element \( z \) of \( \C^n \) is denoted by \( z = (z_0, z_1, \ldots, z_{n-1}) \).
    Define linear functionals \( \omega_0, \omega_1, \ldots, \omega_{n-1} \) on \( \C^n \) by
    \begin{equation*}
        \omega_j(z) = \omega_j(z_0, z_1, \ldots, z_{n-1}) = \frac{1}{\sqrt{n}} \sum_{m=0}^{n-1}
        {z_m e^{(-2 \pi i) j \frac{m}{n}}}.
    \end{equation*}
    The \textit{discrete Fourier transform} is the operator \( \mathcal{F}: \C^n \rightarrow \C^n \)
    defined by
    \begin{equation*}
        \mathcal{F}z = (\omega_0(z), \omega_1(z), \ldots, \omega_{n-1}(z)).
    \end{equation*}
    \begin{enumerate}[label=(\alph*)]
        \item Show that \(  \mathcal{F} \) is a unitary operator on \( \C^n \).
        \item Show that if \( (z_0, z_1, \ldots, z_{n-1}) \in \C^n \) and
            \( z_n \) is defined to equal \( z_0 \), then
            \begin{gather*}
                \mathcal{F}^{-1}  (z_0, z_1, \ldots, z_{n-1}) =
                \mathcal{F} (z_n, z_{n-1}, \ldots, z_{1}).
            \end{gather*}
        \item  Show that \( \mathcal{F}^4 = I \).
    \end{enumerate}
\end{problem}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
    \item Let \( f_0, \ldots, f_{n-1} \) be a standart basis on \( \C^n \). Then
    \begin{gather*}
        \omega_j(f_k) = \frac{1}{\sqrt{n}} \sum_{m=0}^{n-1}
        {\delta_{m, k} e^{(-2 \pi i) j \frac{m}{n}} } =
        \frac{1}{\sqrt{n}} e^{(-2 \pi i) j \frac{k}{n}},\\
        \mathcal{F}(f_k) = (\omega_0(f_k), \omega_1(f_k), \ldots, \omega_{n-1}(f_k)) = \\
        \frac{1}{\sqrt{n}}
        (e^{(-2 \pi i \frac{k}{n}) 0}, e^{(-2 \pi i \frac{k}{n}) 1},
        \ldots, e^{(-2 \pi i \frac{k}{n})(n-1)}).
    \end{gather*}
    Therefore
    \begin{gather*}
        \langle \mathcal{F}(f_k), \mathcal{F}(f_l) \rangle = \\
        %\frac{1}{n}
        %(e^{(-2 \pi i) 0 \frac{k}{n}},
        %\ldots, e^{(-2 \pi i) (n-1) \frac{k}{n}})
        %\cdot
        %(e^{(-2 \pi i) 0 \frac{l}{n}},
        %\ldots, e^{(-2 \pi i) (n-1) \frac{l}{n}})
        %= \\
        \frac{1}{n}
        \sum_{j=0}^{n-1} e^{(-2 \pi i \frac{k}{n})j }
        \overline{e^{(-2 \pi i \frac{l}{n}) j}}
        =
        \frac{1}{n}
        \sum_{j=0}^{n-1} e^{(-2 \pi i \frac{k}{n})j }
        e^{(2 \pi i \frac{l}{n}) j}
        =
        \frac{1}{n}
        \sum_{j=1}^{n-1} e^{(\frac{2 \pi i}{n})j(l-k)}.
    \end{gather*}
    If \( k = l \), then
    \begin{equation*}
        \langle \mathcal{F}(f_k), \mathcal{F}(f_l) \rangle =
        \frac{1}{n}
        \sum_{j=0}^{n-1} e^{0}
        =
        \frac{1}{n}
        \sum_{j=0}^{n-1} 1 = 1.
    \end{equation*}
    Now suppose \( k \ne l \).
    Note that \( e^{\frac{2 \pi i}{n}} \) is the nth root of unity which we denote as~\( \alpha \).
    Then \( \alpha^{(l-k)}  \ne 1 \) is also the nth root of unity,
    therefore \( \alpha^{(l -k)} = e^{2 \pi i \frac{p}{n}} \) for some integer \( p \).
    We have
    \begin{gather*}
        \langle \mathcal{F}(f_k), \mathcal{F}(f_l) \rangle =
        \frac{1}{n}
        \sum_{j=0}^{n-1} \alpha^{(l-k)j}
        =
        \frac{1 - \alpha^{(l-k)n}}{n (1 - \alpha^{(l-k)})}
        =
        \frac{1 - 1}{n (1 - e^{2 \pi i \frac{p}{n}})}
        = 0,
    \end{gather*}
    where the second identity comes from the formula for the partial sum of of the geometric series
    and the third identity comes from
    \begin{equation*}
    \alpha^n =  e^{2 \pi i \frac{p}{n} n } =
    e^{2 \pi i p} = e^0 =~1.
    \end{equation*}
    The equation above implies that
    \( \mathcal{F} f_1, \dots, \mathcal{F} f_n\) is an orthonormal list in \( \C^n \).
    Since it has the length \( n = \C^n \), it is an orthonormal basis of \( \C^n \).
    Thus by 7.53(d) \(  \mathcal{F} \) is unitary.

    \item
    Let \( \alpha = e^{-\frac{2 \pi i}{n}} \).
    We can write the matrix of \( \mathcal{F} \) w.r.t. the standart basis as
    \begin{gather*}
        \frac{1}{\sqrt{n}}
        \begin{pmatrix}
           \alpha^{0 \cdot 0} & \alpha^{0 \cdot 1}  & \ldots & \alpha^{0 \cdot n}  \\
           \alpha^{1 \cdot 0} & \alpha^{1 \cdot 1} & \ldots  & \alpha^{1 \cdot (n-1)} \\
           \vdots  & \ddots &  &  \\
           \alpha^{(n-1) \cdot 0} & \alpha^{(n-1) \cdot 1} & \ldots  & \alpha^{(n-1) \cdot (n-1)}
        \end{pmatrix}
        = \\
        \frac{1}{\sqrt{n}}
        \begin{pmatrix}
           1 & 1 & \ldots & 1  \\
           1 & \alpha & \ldots & \alpha^{(n-1)} \\
           \vdots  & \ddots &  &  \\
           1 & \alpha^{(n-1)} & \ldots  & \alpha^{(n-1) (n-1)}
        \end{pmatrix}.
    \end{gather*}
    Since \( \mathcal{F} \) is unitary, then \( \mathcal{F}^{-1} = \mathcal{F}^* \) and
    the matrix of \( \mathcal{F}^{-1} \) is just the conjugate transpose of the matrix of \( \mathcal{F} \).
    Therefore
    \begin{gather*}
        \mathcal{F}^{-1} (z_0, z_1, \ldots, z_{n-1}) =
        (\overline\omega_0(z), \overline\omega_1(z), \ldots, \overline\omega_{n-1}(z)),
    \end{gather*}
    where
    \begin{gather*}
        \overline\omega_j(z) = \frac{1}{\sqrt{n}} \sum_{m=0}^{n-1}
        {z_m \overline{\alpha^{jm}}}.
    \end{gather*}
    Consider the jth coordinate of \(\mathcal{F} (z_n, z_{n-1}, \ldots, z_{1})\),
    multiplied by \( \sqrt{n} \)
    \begin{gather*}
        {\sqrt{n}} \mathcal{F} (z_n, z_{n-1}, \ldots, z_{1})_{j} =
        \sum_{m=0}^{n-1} z_{n-m} \alpha^{jm}
        \overset{(1)}{=}
        \sum_{k=1}^{n} z_{k} \alpha^{j(n - k)}
        \overset{(2)}{=}  \\
        \sum_{k=1}^{n} z_{k} \alpha^{-jk}
        =
        \sum_{k=1}^{n} z_{k} \overline{\alpha^{jk}}
        =
        \sum_{k=1}^{n-1} z_{k} \overline{\alpha^{jk}} + z_n \overline{\alpha^{jn}}
        \overset{(3)}{=}
        \sum_{k=1}^{n-1} z_{k} \overline{\alpha^{jk}} + z_0 \overline{\alpha^{0}}
        =
        \sum_{k=0}^{n-1} z_{k} \overline{\alpha^{jk}}
        \\ =
        {\sqrt{n}} \mathcal{F}^{-1} (z_0, z_1, \ldots, z_{n-1})_{j},
    \end{gather*}
    in (1) we made the substitution \( k = m - n \) and, correspondingly, \( m = n -k \);
    in (2) and (3) we used \( \alpha^n = 1 \);
    in (3) we used \( z_n = z_0 \).
    Thus \(\mathcal{F}^{-1}  (z_0, z_1, \ldots, z_{n-1}) =
                \mathcal{F} (z_n, z_{n-1}, \ldots, z_{1}) \).
    \item
        Let \( A = \sqrt{n} \mathcal{M}(\mathcal{F})\), \( B  = A^2 \), \( C = B^2 \).
        We will be enumerating matrix rows and columns with \( 0, \dots, n-1 \).
        We have
    \begin{gather*}
        B_{ij} = \sum_{k=0}^{n-1}{A_{ik} A_{kj}}
        = \sum_{k=0}^{n-1}{\alpha^{ik} \alpha^{kj}}
        = \sum_{k=0}^{n-1}(\alpha^{i+j})^{k}.
    \end{gather*}
    We've seen earlier taht the partial sum of this geometric series
    equals 0 if \( \alpha^{i+j} \ne 1 \) and \( n \) otherwise.
    Since \( \alpha \) is the nth root of unity, then \( \alpha^{i+j} = 1 \) iff
    \( i+j \equiv 0  ~\mathrm{mod}~ n \).  Hence
    \begin{gather*}
        B_{ij} =
        \begin{cases}
            n, & \mathrm{if}~i+j \equiv 0  ~\mathrm{mod}~ n, \\
            0, & \mathrm{otherwise}.
        \end{cases}
    \end{gather*}
    Further, we have
    \begin{gather*}
        C_{pq} = \sum_{m=0}^{n-1} B_{pm} B_{mq}.
    \end{gather*}
    Clearly,
    \begin{gather*}
        B_{pm} B_{mq} \ne 0 \iff p+m \equiv 0  ~\mathrm{mod}~ n
        ~\text{and}~ q+m \equiv 0  ~\mathrm{mod}~ n
        \iff p = q = n - m.
    \end{gather*}
    The last identity holds because of our summation limits. Also, for the
    same reason if \( p=q \),
    then only one element in the sum above is nonzero.
    Therefore,
    \begin{gather}
        C_{pq} =
        \begin{cases}
            n^2, & p=q \\
            0, & \mathrm{otherwise}.
        \end{cases}
    \end{gather}
    Finally, we have
    \begin{gather*}
        n^2 I = C = B^2 = A^4 = n^2 \mathcal{M}(\mathcal{F})^4.
    \end{gather*}
    Thus, \(  \mathcal{M}(\mathcal{F})^4 = I\).
    \end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7E %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{7E: Singular Value Decomposition}
\addcontentsline{toc}{section}{7E: Singular Value Decomposition}

\begin{lemma}[properties of \(T^* T\)]
    Suppose \(T \in \Lc(V, W)\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \(T^* T \) is a positive operator on \(V\);
        \item \(\nul T^* T = \nul T\);
        \item \(\range T^* T = \range T^*\);
        \item \(\dim \range T = \dim \range T^* = \dim \range T^* T\).
    \end{enumerate}
\end{lemma}

\begin{definition}[singular values]
    Suppose \(T \in \Lc(V, W)\). The \textbf{singular values} of \(T\) are the nonnegative square
    root of the eigenvalues of \(T^* T\), listed in decreasing order, each included as many
    times as the dimension of the corresponding eigenspace of \(T^*T\).
\end{definition}

\begin{thm}[role of positive singular values]
    Suppose that \(T \in \Lc(V, W)\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \(T\) is injective \(\Longleftrightarrow\) 0 is not a singular value of \(T\);
        \item the number of positive singular values of \(T\) equals \(\dim \range T\);
        \item \(T\) is surjective \(\Longleftrightarrow\) number of positive singular values of \(T\) equals \(\dim W\).
    \end{enumerate}
\end{thm}

\begin{corollary}
    Suppose \(S \in \Lc(V, W)\). Then
    \[S \text{ is an isometry } \Longleftrightarrow \text{ all singular values of } S \text{ equal 1.}\]
\end{corollary}

\begin{thm}[\textcolor{red}{SINGULAR VALUE DECOMPOSITION}]
    Suppose \(T \in \Lc(V, W)\) and the positive singular values of \(T\) are \(s_1, \ldots, s_m\).
    Then there exist orthonormal lists \(e_1, \ldots, e_m\) in \(V\) and \(f_1, \ldots, f_m\) in \(W\)
    such that
    \[Tv = s_1 \langle v,e_1 \rangle f_1 + \cdots + s_m \langle v,e_m \rangle f_m\]
    for every \(v \in V\).
\end{thm}

\begin{remark}
    In the proof, we first let \(s_1, \ldots, s_n\) to denote the singular values of \(T\) (thus
    \(n = \dim V\)). Then the spectral theorem implies that there exists an orthonormal list \(e_1, \ldots, e_n\)
    of \(V\) with
    \[T^* T e_k = s_k^2 e_k\]
    For each \(k = 1, \ldots, m\), define
    \[f_k = \frac{T e_k}{s_k}\]
\end{remark}

\begin{remark}
    Suppose \(T \in \Lc(V, W)\), the positive singular values of \(T\) are \(s_1, \ldots, s_m\)
    and \(e_1, \ldots, e_m\) and \(f_1, \ldots, f_m\) are as in the singular decomposition above.
    Then the two orthonormal lists can both be extended to basis of the respective vector space.  Where
    we can now define
    \[Te_k = \begin{cases}
        s_k f_k &\text{if } 1 \leq k \leq m, \\
        0 &\text{if } m < k \leq \dim V
    \end{cases}\]
\end{remark}

\begin{definition}[diagonal matrix]
    An \(M\)-by-\(N\) matrix \(A\) is called a \textbf{diagonal matrix} if all entries of the matrix are 0
    except possibly \(A_{k, k}\) for \(k = 1, \ldots \min \{M, N\}\).
\end{definition}

\begin{thm}[singular value decomposition of adjoint and pseudoinverse]
    Suppose \(T \in \Lc(V, W)\) and the positive singular values of \(T\) are
    \(s_1, \ldots, s_m\). Suppose \(e_1, \ldots, e_m\) and \(f_1, \ldots, f_m\) are orthonormal
    list in \(V\) and \(W\) such that
    \[Tv = s_1 \langle v,e_1 \rangle f_1 + \cdots + s_m \langle v,e_m \rangle f_m\]
    for every \(v \in V\). Then
    \[T^*w = s_1 \langle w,f_1 \rangle e_1 + s_m \langle w,f_m \rangle e_m\]
    and
    \[T^\dagger w = \frac{\langle w,f_1 \rangle}{s_1}e_1 + \cdots + \frac{\langle w,f_m \rangle}{s_m}e_m\]
    for every \(w \in W\).
\end{thm}

\begin{thm}[matrix version of SVD]
    Suppose \(A\) is a \(p\)-by-\(n\) matrix of rank \(m \geq 1\). Then there exist a \(p\)-by-\(m\) matrix
    \(B\) with orthonormal columns, an \(m\)-by-\(m\) diagonal matrix \(D\) with positive numbers on
    the diagonal, and an \(n\)-by-\(m\) matrix \(C\) with orthonormal columns such that
    \[A = BDC^*\]
\end{thm}

\begin{remark}
    \(A\) is a \(p \times n\) matrix while \(BDC^*\) has a total of \(m(p+m+n)\) entries, which
    could be considerably smaller than \(A\).
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7E PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{7E Problem Sets}

\begin{problem}{1}
    Suppose \(T \in \Lc(V, W)\). Show that \(T = 0\) if and only if all singular
    values of \(T\) are 0.
\end{problem}

\begin{proof}
\(\Rightarrow\) if \(T = 0\), then \(T^*T = 0\) and the singular values have to be all 0.

\(\Leftarrow\) if all singular values are 0, then \(\dim \range T = 0\) and thus \(T = 0\).
\end{proof}

\begin{problem}{2}
    Suppose \(T \in \Lc(V, W)\) and \(s > 0\). Prove that \(s\) is a singular value of \(T\)
    if and only if there exists nonzero vectors \(v \in V\) and \(w \in W\) such that
    \[Tv = sw \text{ and } T^* w = sv\]
\end{problem}

\begin{proof}

\(\Rightarrow\) This means that there exists nonzero eigenvector \(v \in V\) such that
\[T^*T v = s^2 v\]
let \(w = \frac{Tv}{s}\), then
\[Tv = s\left(\frac{Tv}{s}\right) = sw\]
and
\[T^*w  = \frac{T^* Tv}{s} = sv\]

\(\Leftarrow\) we have that
\[T^*Tv = T^*(sw) = s^2v\]
Therefore, completing the proof.
\end{proof}

\begin{problem}{4}
    Suppose that \(T \in \Lc(V, W)\), \(s_1\) is the largest singular value of \(T\), and
    \(s_n\) is the smallest value of \(T\). Prove that
    \[\{\norm{Tv} \colon v \in V \text{ and } \norm{v} = 1\} = [s_n, s_1]\]
\end{problem}

\begin{proof}
Our proof goes two-fold. First we prove that for any \(v \in V\) s.t. \(\norm{v} = 1\), we have
\[s_1 \leq \norm{Tv} \leq s_n\]
Second, we will show that the \(A \coloneqq \{\norm{Tv} \colon v \in V \text{ and } \norm{v} = 1\} \)
is a closed interval.

To prove the first part, let \(v\) be a unit norm vector in \(V\), we know that
\(v = \sum_{i=1}^{n}a_i e_i\) and \(\norm{v}^2 = \sum_{i=1}^{n} |a_i|^2\) for some orthonormal
basis \(e_1, \ldots, e_n\). Then we have
\begin{align*}
    \norm{Tv}^2
    &= \langle Tv,Tv \rangle \\
    &= \langle v,T^*Tv \rangle \\
    &= \left\langle \sum_{i=1}^{n} a_i e_i, \sum_{i=1}^{n} s_i^2a_ie_i \right\rangle \\
    &= \sum_{i=1}^{n} a_i \sum_{j=1}^{n} \overline{a_j} \overline{s_j^2} \langle e_i,e_j \rangle \\
    &= \sum_{i=1}^{n} |a_i|^2 s_i^2 \\
\end{align*}
Note that the last identity is \(\leq \norm{v}^2 s_1^2\) and \(\geq \norm{v}^2 s_n^2\), which gets
our desired equality. For the second part, we know that the norm function \(\norm{\cdot} \colon
v \to Tv\) is continuous on the unit sphere \(\{v \in V \colon \norm{v}  =1\}\), which is compact. The
continuous image of the compact set is therefore also compact. Hence, we prove that
\[A = [s_n, s_1]\]
\end{proof}

% \begin{problem}{8}
%     Suppose \(T \in \Lc(V, W)\). Suppose \(s_1 \geq s_2 \geq \cdots \geq s_m > 0\)
%     and \(e_1, \ldots, e_m\) is an orthonormal list in \(V\) and \(f_1, \ldots, f_m\)
%     is an orthonormal list in \(W\) such that
%     \[Tv = s_1 \langle v,e_1 \rangle f_1 + \cdots + s_m \langle v,e_m \rangle f_m\]
%     for every \(v \in V\).
%     \begin{enumerate}[label=(\alph*)]
%         \item Prove that \(f_1, \ldots, f_m\) is an orthonormal basis of \(\range T\).
%         \item Prove that \(e_1, \ldots, e_m\) is an orthonormal basis of \((\nul T)^\perp\).
%         \item Prove that \(s_1, \ldots, s_m\) are positive singular values of \(T\).
%         \item Prove that if \(k \in \{1, \ldots, m\}\), then \(e_k\) is an eigenvector of
%         \(T^*T\) with corresponding eigenvalue \(s_k^2\).
%         \item Prove that
%         \[TT^* w = s_1^2 \langle w,f_1 \rangle f_1 + \cdots + s_m^2 \langle w,f_m \rangle f_m\]
%         for all \(w \in W\).
%     \end{enumerate}
% \end{problem}

% \begin{proof}

% \end{proof}

\begin{problem}{9}
    Suppose \(T \in \Lc(V, W)\). Show that \(T\) and \(T^*\) have the same positive singular
    values.
\end{problem}

\begin{proof}
We know that \(T^*T\) is self-adjoint, and that its eigenvalue equals the eigenvalue of its adjoint.
Therefore the squared root of the eigenvalue of \(T^*T = TT^*\), and thus \(T\) and \(T^*\) have
the same positive singular values.

Problem 10 follows similarly.
\end{proof}

\begin{problem}{11}
    Suppose that \(T \in \Lc(V, W)\) and \(v_1, \ldots, v_n\) is an orthonormal basis of \(V\).
    Let \(s_1, \ldots, s_n\) denote the singular values of \(T\).
    \begin{enumerate}[label=(\alph*)]
        \item Prove that \(\norm{Tv_1}^2 + \cdots + \norm{Tv_n}^2 = s_1^2 + \cdots + s_n^2\).
        \item Prove that if \(W = V\) and \(T\) is a positive operator, then
        \[\langle Tv_1,v_1 \rangle + \cdots + \langle Tv_n,v_n \rangle = s_1 + \cdots + s_n\]
    \end{enumerate}
\end{problem}

\begin{proof}
We first know from the previous problems that for any two orthonormal basis, \(e_1, \ldots, e_n\)
in \(V\) and \(f_1, \ldots, f_m\) in \(W\), then
\[\sum_{i=1}^{n} \norm{Te_i}^2 = \sum_{j=1}^{m} \norm{T^* f_j}^2\]

(a) In the proof of SVD, we have shown that by letting \(f_k = \frac{Te_k}{s_k}\), then we have
\(f_1, \ldots, f_n\) also to be an orthonormal list in \(W\). Then applying the fact above yields
that
\[\sum_{i=1}^{n} \norm{T v_i}^2 = \sum_{i=1}^{n} \norm{T^*f_i}^2 =
\sum_{i=1}^{n} \norm{ \frac{T^*T e_i}{s_i}}^2 = \sum_{i=1}^{n} \norm{s_i e_i}^2 = \sum_{i=1}^{n} s_i^2\]

where \(e_1, \ldots, e_n\) are the orthonormal eigenbasis that diagonalizes \(T^*T\).

(b) Note that since \(T\) is positive, the singular value is eigenvalue, as \(\lambda(T^*T)
= \lambda(T^2)\) and the eigenvalue is positive. Therefore, for each \(i\), \(T v_i = s_i v_i\)
and we have that
\[\sum_{i=1}^{n} \langle Tv_i,v_i \rangle = \sum_{i=1}^{n} s_i\]
\end{proof}

\begin{problem}{13}
    Suppose \(T_1, T_2 \in \Lc(V)\). Prove that \(T_1\) and \(T_2\) have the same singular values
    if and only if there exist unitary operators \(S_1, S_2 \in \Lc(V)\) such that \(T_1
    = S_1 T_2 S_2\).
\end{problem}

\begin{proof}
\(\Rightarrow\) Let \(A\) be the matrix of \(T_2\) and \(B\) be the matrix of \(T_1\) then we know that
\(A = U_1 D V_1^*\) and \(B = U_2 D V_2^*\) for unitary matrix \(U_1, V_1, U_2, V_2\) and
diagonal matrix \(D\). Then we have
\[B = \underbrace{(U_1 U_2^*)}_{S_1} \underbrace{(U_2 D V_2^*)}_A \underbrace{(V_2 V_1^*)}_{S_2} \]

where we have \(S_1, S_2\) to be unitary.

\(\Leftarrow\) Let \(A\) be the matrix of \(T_2\) and let \(A = UD^*\) be the SVD of \(A\) where
\(U\) and \(V^*\) is unitary, then we have that (let \(B\) be the matrix of \(T_1\))
\[B = (S_1 U)D(V^*S_2)\]
where \(S_1 U\) and \(V^*S_2\) are both unitary, therefore \(B\) (\(T_2\)) also have the singular values
as in the diagonal of \(D\). Therefore, \(T_1\) and \(T_2\) shares same eigenvalues.
\end{proof}

\begin{problem}{15}
    Suppose \(T \in \Lc(V)\) and \(s_1 \geq \cdots \geq s_n\) are the singular values of \(T\).
    Prove that if \(\lambda\) is an eigenvalue of \(T\), then \(s_1 \geq |\lambda| \geq s_n\).
\end{problem}

\begin{proof}
In the proof of Problem 4, we have shown that for every \(v \in V\), we have
\[ s_n \norm{v} \leq \norm{Tv} \leq s_1 \norm{v}\]
Since this holds for all vectors, substitute any eigenvector \(v\) we can get that
\[s_n \norm{v} \leq |\lambda| \norm{v} \leq s_1 \norm{v}\]
which is the desired result.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7F %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section*{7F: Consequences of Singular Value Decomposition}
\addcontentsline{toc}{section}{7F: Consequences of Singular Value Decomposition}

\begin{thm}[upper bound for \(\norm{Tv}\)]
    Suppose \(T \in \Lc(V, W)\). Let \(s_1\) be the largest singular value of \(T\). Then
    \[\norm{Tv} \leq s_1 \norm{v}\]
    for all \(v \in V\).
\end{thm}

\begin{definition}[norm of a linear map, \(\norm{\cdot}\)]
    Suppose \(T \in \Lc(V, W)\). Then the \textbf{norm} of \(T\), denoted by \(\norm{T}\),
    is defined by
    \[\norm{T} = \max \{\norm{Tv} \colon v \in V \text{ and } \norm{v} \leq 1\}\]
\end{definition}

\begin{remark}
    For a linear map \(T\), \(\norm{T} = \sigma_{\max}\) (using the more common notation). Also
    note that \(\norm{T} \neq \sqrt{\langle T,T \rangle}\). Now we have two different uses of
    the word \textbf{norm} and the notation \(\norm{\cdot}\).
\end{remark}

\begin{corollary}[basic properties of norms of linear maps]
    Suppose \(T \in \Lc(V, W)\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \(\norm{T} \geq 0\);
        \item \(\norm{T} = 0 \Longleftrightarrow T = 0\);
        \item \(\norm{\lambda T} = |\lambda| \norm{T}\) for all \(\lambda \in \F\);
        \item \(\norm{S+T} \leq \norm{S} + \norm{T}\) for all \(S \in \Lc(V, W)\).
    \end{enumerate}
\end{corollary}

\begin{remark}
    For \(S, T \in \Lc(V, W)\), the quantity \(\norm{S - T}\) is often called the distance between
    \(S\) and \(T\). Informally, think of the condition that \(\norm{S - T}\) is a small number
    as meaning \(S\) and \(T\) are close together.
\end{remark}

\begin{thm}[alternative formulas for \(\norm{T}\)]
    Suppose \(T \in \Lc(V, W)\). Then
    \begin{enumerate}[label=(\alph*)]
        \item \(\norm{T}\) = the largest eigenvalue of \(T\);
        \item \(\norm{T} = \max \{\norm{Tv} \colon v \in V \text{ and } \norm{v} = 1\} \);
        \item \(\norm{T} = \text{ the smallest number } c \text{ such that } \norm{Tv} \leq c \norm{v}\)
        for all \(v \in V\).
    \end{enumerate}
\end{thm}

\begin{remark}
    An important inequality during the proof:
    \[\norm{Tv} \leq \norm{T} \norm{v}\]
    for all \(v \in V\) and \(v \neq 0\).
\end{remark}

\begin{corollary}[norm of the adjoint]
    Suppose \(T \in \Lc(V, W)\). Then \(\norm{T} = \norm{T^*}\).
\end{corollary}

\begin{thm}[best approximation by linear map whose range has dimension \(\leq k\)]
Suppose \(T \in \Lc(V, W)\) and \(s_1 \geq \cdots \geq s_m\) are the singular
values of \(T\). Suppose \(1 \leq k < m\). Then
\[\min \{\norm{T - S} \colon S \in \Lc(V, W) \text{ and }
\dim \range S \leq k\} = s_{k+1}\]
Furthermore, if
\[Tv = s_1 \langle v,e_1 \rangle f_1 + \cdots + s_m \langle v,e_m \rangle f_m\]
is a singular value decomposition of \(T\) and \(T_k \in \Lc(V, W)\) is
defined by
\[T_k v = s_1 \langle v,e_1 \rangle f_1 + \cdots + s_k \langle v,e_k \rangle f_k\]
for each \(v \in V\), then \(\dim \range T_k = k\) and \(\norm{T - T_k} = s_{k+1}\).
\end{thm}

\begin{thm}[polar decomposition]
    Suppose \(T \in \Lc(V)\). Then there exists a unitary operator
    \(S \in \Lc(V)\) such that
    \[T = S \sqrt{T^* T}\]
\end{thm}

\begin{remark}
    This holds for both \(\C\) and \(\R\).
\end{remark}

\begin{definition}[ball, \(B\)]
    The \textbf{ball} in \(V\) of radius \(1\) centered at 0, is defined by
    \[B = \{v \in V \colon \norm{v} \leq 1\}\]
\end{definition}

\begin{definition}[ellipsoid, \(E(s_1f_1, \ldots, s_n f_n)\), principle axes]
    Suppose that \(f_1, \ldots, f_n\) is an orthonormal basis of \(V\) and
    \(s_1, \ldots, s_n\) are positive numbers. The \textbf{ellipsoid}
    \(E(s_1 f_1, \ldots, s_n f_n)\) with \textbf{principle axes} \(s_1 f_1,
    \ldots, s_n f_n\) is defined by
    \[E(s_1 f_1, \ldots, s_n f_n) = \{v \in V \colon
    \frac{|\langle v,f_1 \rangle|^2}{s_1}
    + \cdots + \frac{|\langle v,f_n \rangle|^2}{s_n} < 1\}\]
\end{definition}

\begin{remark}
    If \(\dim V = 2\), the word ``disk'' is sometimes used to denote ball and
    the word ``ellipse'' is sometimes used to denote ellipsoid.
\end{remark}

\begin{definition}[\(T(\Omega)\)]
    For a function \(T\) defined on \(V\) and \(\Omega \subseteq V\), define
    \(T(\Omega)\) by
    \[T(\Omega) = \{Tv \colon v \in \Omega\}\]
\end{definition}

\begin{proposition}[invertible map takes ball to ellipsoid]
    Suppose \(T \in \Lc(V)\) is invertible. Then \(T\) maps
    the ball \(B\) in \(V\) onto an ellipsoid in \(V\).
\end{proposition}

\begin{proposition}[invertible map takes ellipsoid to ellipsoid]
    Suppose \(T \in \Lc(V)\) is invertible and \(E\) is an ellipsoid in
    \(V\). Then \(T(E)\) is an ellipsoid in \(V\).
\end{proposition}

\begin{definition}[\(P(v_1, \ldots, v_n)\), parallelepiped]
    Suppose \(v_1, \ldots, v_n\) is a basis of \(V\). Let
    \[P(v_1, \ldots, v_n) = \{a_1 v_1 + \cdots + a_n v_n \colon a_1, \ldots, a_n \in (0,1)\}\]
    A \textbf{parallelepiped} is a set of the form \(u + P(v_1, \ldots, v_n)\)
    for some \(u \in V\). The vectors \(v_1, \ldots, v_n\) are called
    the \textbf{edges} of the parallelepiped.
\end{definition}

\begin{proposition}[invertible operator takes parallelepiped to parallelepiped]
    Suppose \(u \in V\) and \(v_1, \ldots, v_n \) is a basis of \(V\). Suppose
    \(T \in \Lc(V)\) is invertible. Then
    \[T(u + P(v_1, \ldots, v_n)) = Tu + P(Tv_1 \ldots, Tv_n)\]
\end{proposition}

\begin{definition}[box]
    A \textbf{box} is of the form
    \[u + P(r_1 e_1, \ldots, r_n e_n)\]
    where \(u \in V\), \(r_1, \ldots, r_n\) are positive numbers and
    \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\).
\end{definition}

Rest of Notes are omitted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 7F PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{7F Problem Sets}

\begin{problem}{1}
    Prove that if \(S, T \in \Lc(V, W)\), then
    \(|\norm{S} - \norm{T}| \leq \norm{S - T}\).
\end{problem}

\begin{proof}
We have
\begin{align*}
    \norm{S} &= \norm{S - T + T} \leq \norm{S - T} + \norm{T} \\
    \norm{T} &= \norm{T - S + S} \leq \norm{T - S} + \norm{S}
\end{align*}
This means that
\[\norm{S - T} \geq |\norm{S} - \norm{T}|\]
\end{proof}

\begin{problem}{2}
    Suppose that \(T \in \Lc(V)\) is self-adjoint and
    or that \(\F = \C\) and \(T \in \Lc(V)\) is normal, then
    \[\norm{T} = \{\max |\lambda| \colon \lambda
    \text{ is an eigenvalue of } T\}\]
\end{problem}

\begin{proof}
Given by the conditions we know that the eigenvalues of
\(T\) are its singular values, therefore
\(\norm{T} = s_1 = \max(\lambda)\)
\end{proof}

\begin{problem}{3}
    Suppose that \(T \in \Lc(V)\) and \(v \in V\). Prove that
    \[\norm{Tv} = \norm{T} \norm{v} \Longleftrightarrow
    T^* T v = \norm{T}^2 v\]
\end{problem}

\begin{proof}
\(\Rightarrow\) We have that
\(\norm{Tv} = \norm{Tv / \norm{v}} \norm{v} = \norm{T} \norm{v}\).
Therefore \(\norm{Tv / \norm{v}} = \norm{T}\) which
gives that \(v/\norm{v}\) being the vector corresponds to the largest
singular value of \(T\). This gives that
\[T^*T v/\norm{v} = \norm{T^2}v / \norm{v}\]

Multiplying \(\norm{v}\) on both sides solves the problem.

\(\Leftarrow\) Given \(T^*Tv = \norm{T}^2 v\), then we know
that \(v\) is the eigenvector of \(T^*T\) corresponding to
its largest eigenvalue \(s_1^2\), (i.e. it is the vector
that corresponds the largest singular value of \(T\)).
Therefore, we have \(\norm{Tv} =
\norm{Tv / \norm{v}} \norm{v}= \norm{T} \norm{v}\).
\end{proof}

\begin{problem}{4}
    Suppose \(T \in \Lc(V, W), v \in V\), and
    \(\norm{Tv} = \norm{T} \norm{v}\). Prove that if
    \(u \in V\) and \(\langle u,v \rangle = 0\), then
    \(\langle Tu,Tv \rangle = 0\).
\end{problem}

\begin{proof}
By P3 we know that \(T^*T v = \norm{T}^2 v\), then
\[\langle Tu,Tv \rangle
= \langle T^*Tu,v \rangle = \norm{T}^2 \langle u,v \rangle = 0\]
\end{proof}

\begin{problem}{5}
    Suppose \(U\) is a finite-dimensional inner product
    space, \(T \in \Lc(V, U)\), and \(S \in \Lc(U, W)\).
    Prove that
    \[\norm{ST} \leq \norm{S} \norm{T}\]
\end{problem}

\begin{proof}
Take \(v \in V\) with \(\norm{v} \leq 1\) and
\(\norm{ST} = \norm{ST v}\), then
\begin{align*}
    \norm{ST}
    = \norm{(ST)v}
    = \norm{S(Tv)}
    \leq \norm{S}\norm{Tv}
    \leq \norm{S} \norm{T} \norm{v}
    \leq \norm{S} \norm{T}
\end{align*}
\end{proof}

\begin{problem}{7}
    Show that defining \(d(S, T) = \norm{S - T}\) for
    \(S, T \in \Lc(V, W)\) makes \(d\) a metric on
    \(\Lc(V, W)\).
\end{problem}

\begin{proof}
We will examine the definitions one by one:
\begin{itemize}
    \item \(d(S, S) = \norm{S - S} = 0\).
    \item If \(S \neq T\), then \(d(S, T)
    = \norm{S - T} = \sigma_{\max}(S - T) \). Since
    \(S - T \neq 0\), its largest singular value is nonzero
    and therefore \(d(S, T) > 0\).
    \item \(d(S, T) = \norm{S - T} = \norm{T - S} =
    d(T, S)\).
    \item \(d(S, G) = \norm{S - G}
    = \norm{(S - T) + (T - G)}
    \leq \norm{S - T} + \norm{T - G}
    = d(S, T) + d(T, G)\).
\end{itemize}
\end{proof}

\begin{problem}{8}
    \begin{enumerate}[label=(\alph*)]
        \item Prove that if \(T \in \Lc(V)\) and
        \(\norm{I - T} < 1\), then \(T\) is invertible.
        \item Suppose that \(S \in \Lc(V)\) is invertible.
        Prove that if \(T \in \Lc(V)\) and
        \(\norm{S - T} < 1/\norm{S^{-1}}\), then
        \(T\) is invertible.
    \end{enumerate}
\end{problem}

\begin{proof}
% (a) We know that
% \[1 > \norm{I - T} \geq |\norm{I} - \norm{T}| = |1 - \norm{T}|\]
% This gives that
% \[0 < \norm{T} < 1\]
(a) Let \(S = I - T\) and \(S_n = \sum_{i=1}^{n} S^{i}\).
Then
\[(I - S)S_n = I - S^{n+1} \to I\]
as \(n \to \infty\) because \(\norm{S} < 1\). Therefore, the operator
\[S_\infty = \sum_{i=1}^{\infty} S_i\]
is also bounded. This gives that
\[(I - S)S_{\infty} = S_{\infty} (I - S) = I\]
and thus \(S\) is invertible, which leads to that \(T\) is also invertible.

(b) Equivalently,
\[ \norm{I - TS^{-1}}  \leq \norm{S - T} \norm{S^{-1}} < 1\]
So \(TS^{-1}\) is invertible. Since we already know that
\(S\) is invertible, \(T\) is therefore also invertbile.
\end{proof}

\begin{problem}{9}
    Suppose \(T \in \Lc(V)\). Prove that for every \(\epsilon > 0\)
    there exists an invertible operator \(S \in \Lc(V)\)
    such that \(0 < \norm{T - S } < \epsilon\).
\end{problem}

\begin{proof}
Define \(S = T + \delta I\) for some \(\epsilon > \delta > 0\). Then we
have
\[\norm{T - S} = \norm{\delta I} = \delta \]
which satisifes the desired condition. Note that if \(T\) is invertible, we can
simply choose a sufficiently small \(\delta < 1/\norm{T^{-1}}\); if not, then any
\(\delta\) in \((0, 1)\) can make \(S\) invertible.
\end{proof}

\begin{problem}{12}
    Suppose \(T \in \Lc(V)\) is a positive operator. Show
    that \(\norm{\sqrt{T}} = \sqrt{\norm{T}}\).
\end{problem}

\begin{proof}
Let \(\norm{T} = \sigma_1\), then \(\norm{\sqrt{T}}
= \sqrt{\sigma_1} = \sqrt{\norm{T}}\).
\end{proof}

\begin{problem}{17}
    Prove that if \(u \in V\) and \(\varphi_u\) is the linear functional on \(V\) defined
    by the equation \(\varphi_u(v) = \langle v,u \rangle\), then \(\norm{\varphi_u} = \norm{u}\).
\end{problem}

\begin{proof}
We have that
\[\norm{\varphi_u} = \sup_{\norm{v} = 1} |\langle v,u \rangle| = \left|\left\langle \frac{u}{\norm{u}},u \right\rangle \right| = \norm{u}\]
\end{proof}

\begin{problem}{18}
    Suppose \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\) and \(T \in \Lc(V, W)\).
    \begin{enumerate}[label=(\alph*)]
        \item Prove that \(\max\{\norm{Te_1}, \ldots, \norm{Te_n}\}\leq \norm{T} \leq (\norm{Te_1}^2 + \cdots + \norm{Te_n}^2)^{1/2}\).
        \item Prove that \(\norm{T} = (\norm{T e_1}^2 + \cdots + \norm{Te_n}^2)^{1/2}\) if and only
        if \(\dim \range T \leq 1\).
    \end{enumerate}
\end{problem}

\begin{proof}
(a) We first know that \(\norm{T e_i} \leq \norm{T}\) by definition of \(\norm{\cdot}\). In the proof
of equivalent characterizations, we showed that there exists some orthonormal basis (corresponding to SVD)
such that \(\norm{T(f_n)} = s_1\) for some \(f_n\). This finishes the l.h.s. To see the r.h.s.,
let \(v \in V\) with \(\norm{v} = 1\) to be such that \(\norm{Tv} = \norm{T}\). Then we have
\begin{align*}
    \norm{T}^2 = \norm{Tv}^2 = \norm{ \sum_{i=1}^{n} a_i  Te_i}^2
    \leq \left(\sum_{i=1}^{n} |a_i|^2 \right) \left(\sum_{i=1}^{n} \norm{Te_i}^2 \right)
    = \sum_{i=1}^{n} \norm{Te_i}^2
\end{align*}

Taking the square root solves the problem.

(b) \(\Rightarrow\) By the Cauchy-Schwarz inequality, we know that this is an equality if and only if
\(Te_i\) must be proportional to each other. That is, there exists
a unit vector \(w\) such that \(Te_i = a_i w\). So \(\dim \range T \leq 1\).

\(\Leftarrow\) If \(\dim \range T \leq 1\), then there exists a unit vector \(w\) and
\(\beta_i \geq 0\) s.t.
\[Te_i = \beta_i w\]
We now have that for some \(v\) with unit norm,
\[\norm{Tv} = \norm{\sum_{i=1}^{n} a_i T e_i} =
\norm{\sum_{i=1}^{n}a_i \beta_i w} \leq \left|\sum_{i=1}^{n} a_i \beta_i \right|\]

We know that
\[\left|\sum_{i=1}^{n} a_i \beta_i \right| \leq \left(\sum_{i=1}^{n} a_i^2 \right)^{1/2} \left(\sum_{i=1}^{n} \beta_i^2 \right)^{1/2}
= \left(\sum_{i=1}^{n} \beta_i^2 \right)^{1/2}\]
with equality obtained when \(\beta_i = c a_i\) for all \(i\). That is, the sup of \(\norm{Tv}\)
occurs under this condition and that we have
\[\norm{T} = \left(\sum_{i=1}^{n} \beta_i^2 \right)^{1/2} = \left(\sum_{i=1}^{n} \norm{Te_i}^2 \right)^{1/2}\]
\end{proof}

\begin{problem}{24}
    Suppose \(T \in \Lc(V)\) is invertible. Prove that
    \[\norm{T^{-1}} = \norm{T}^{-1} \Longleftrightarrow \frac{T}{\norm{T}} \text{ is a unitary operator}\]
\end{problem}

\begin{proof}
\(\Rightarrow\) Let \(U = \frac{T}{\norm{T}}\), then \(\norm{U} = 1\) and \(\norm{U^{-1}} = 1\). Then
\[\norm{Ux} \leq \norm{U}\norm{x} = \norm{x}\]
and
\[\norm{x} = \norm{U^{-1}Ux} \leq \norm{U^{-1}}\norm{Ux} = \norm{Ux}\]
Therefore, we have \(\norm{Ux} = \norm{x}\) and therefore it is an invertible isometry and thus unitary.

\(\Leftarrow\) Conversely, we have \(\norm{U} = 1 = \norm{U^{-1}}\) by it being unitary. Therefore, we have
\(\norm{T^{-1}} = \norm{T}^{-1}\)
\end{proof}

\end{document}
